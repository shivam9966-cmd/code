from mssparkutils.fs import exists, rm

output_path = "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single"

# Delete only if folder exists
if exists(output_path):
    rm(output_path, recurse=True)


df_single.write.mode("overwrite").option("header", True).csv(output_path)



# 1. Read your multipart folder
df = (
    spark.read
    .option("header", True)
    .csv("Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset/*")
)

# 2. Merge into 1 CSV
df_single = df.coalesce(1)

# 3. Safe delete (won’t throw error if folder doesn’t exist)
from mssparkutils.fs import exists, rm

output_path = "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single"

if exists(output_path):
    rm(output_path, recurse=True)

# 4. Write as single CSV
df_single.write.mode("overwrite").option("header", True).csv(output_path)
