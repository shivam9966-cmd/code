
from pyspark.sql import functions as F, Window as W
from functools import reduce

HIST_WINDOW = 14
K_FACTOR = 3.0

ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

GROUP_COLS = ACCOUNT_LEVELS + PRODUCT_LEVELS + COSTCENTER_LEVELS
KPIS = ["EBITDA","OPEX","Revenue"]

def _value_block(cols): return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])
def _full_value_path(): return F.concat_ws("-**-", _value_block(ACCOUNT_LEVELS), _value_block(PRODUCT_LEVELS), _value_block(COSTCENTER_LEVELS))
def _aan_key(metric_name_lit): return F.concat_ws("::", metric_name_lit, _full_value_path())

# --- recompute rolling stats per KPI ---
def recompute_stats(df_base, kpi_label):
    from pyspark.sql import Window as W
    base = df_base.filter(F.lower("account_L5_Description").contains(kpi_label.lower()))
    base = base.withColumn("aan_key", _aan_key(F.lit(kpi_label)))
    hist_w = (W.partitionBy(*GROUP_COLS).orderBy("pred_date").rowsBetween(-HIST_WINDOW, -1))

    base = (base
        .withColumn("rolling_mean", F.avg("Amount").over(hist_w))
        .withColumn("rolling_std",  F.stddev_samp("Amount").over(hist_w))
        .withColumn("lower_band",   F.col("rolling_mean") - F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("upper_band",   F.col("rolling_mean") + F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("recalc_delta",
                    F.when(F.col("rolling_mean") != 0,
                           (F.col("Amount") - F.col("rolling_mean")) / F.abs(F.col("rolling_mean")) * 100))
        .select("aan_key","pred_date","Amount","rolling_mean","rolling_std",
                "lower_band","upper_band","recalc_delta")
        .withColumn("aan_name", F.lit(kpi_label))
    )
    return base

recomputed_df = reduce(DataFrame.unionByName, [recompute_stats(df, k) for k in KPIS])

# --- Join with anomalies_df ---
validated_df = (anomalies_df.alias("a")
                .join(recomputed_df.alias("r"),
                      (F.col("a.aan") == F.col("r.aan_key")) &
                      (F.col("a.predictive_date") == F.col("r.pred_date")) &
                      (F.col("a.aan_name") == F.col("r.aan_name")),
                      "left")
                .withColumn("delta_diff", F.round(F.col("r.recalc_delta") - F.col("a.aan_delta").cast("double"), 3))
                .withColumn("expected_diff", F.round(F.col("r.rolling_mean") - F.col("a.aan_attr"), 3))
                .withColumn("outside_band", (F.col("r.Amount") < F.col("r.lower_band")) | (F.col("r.Amount") > F.col("r.upper_band")))
                .withColumn("validated", (F.abs(F.col("delta_diff")) < 0.05) & F.col("outside_band"))
)

# --- Summary of validation ---
summary = (validated_df.groupBy("a.aan_name")
           .agg(F.count("*").alias("total_anoms"),
                F.sum(F.when(F.col("validated"),1).otherwise(0)).alias("verified_anoms"),
                (F.sum(F.when(F.col("validated"),1).otherwise(0))/F.count("*")*100).alias("verified_pct"))
           .orderBy("a.aan_name"))

display(summary)

# --- Optional: check mismatches ---
display(validated_df.filter(~F.col("validated")).limit(100))
