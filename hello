import pyspark.sql.functions as F

# Load your fact and dimension tables
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")
account_df  = spark.table("FPNA_FISRPT_SILVER.dbo.account")

# Join helper function to bring KPI info
def add_kpi_flag(df):
    joined = df.join(account_df, df["ACCOUNT"] == account_df["Account"], "left")
    return joined.withColumn(
        "KPI",
        F.when(F.col("L5_Description") == "Revenue", "Revenue")
         .when(F.col("L5_Description") == "Operating Expenses", "OPEX")
         .when(F.col("L4") == "EBITDA", "EBITDA")
    ).filter(F.col("KPI").isNotNull())

# Apply to each fact table
act_kpi = add_kpi_flag(actual_df)
pln_kpi = add_kpi_flag(plan_df)
fcst_kpi = add_kpi_flag(forecast_df)

# Show counts
print("KPI Row Counts per Table:\n")
print(f"ACTUALS  → {act_kpi.count()}")
print(f"PLAN     → {pln_kpi.count()}")
print(f"FORECAST → {fcst_kpi.count()}")
