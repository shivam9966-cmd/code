# ============================================================
# FINAL SCRIPT — Filter + Dedup + Concatenate + Save
# (No default FY25 filter)
# ============================================================

import pyspark.sql.functions as F
from functools import reduce

# -----------------------------
# CONFIGURATION
# -----------------------------
TARGET_PATH = "/lakehouse/default/Files/SQL_DUMPS/FACTS_CONCAT_DEDUP_FILTERED"
OUTPUT_FORMAT = "parquet"          # or "delta" / "csv"
COALESCE_SINGLE_FILE = True        # single file output for easy export
ADD_FACT_SOURCE = True             # adds column to track origin table (Actual/Plan/Forecast)

# -----------------------------
# FRONT-END FILTER PLACEHOLDER
# -----------------------------
# You can later populate this dict from Power BI, front-end, or pipeline variables
# Keep all None for now → includes everything (no filter)
filter_dict = {
    "YEAR": None,          # e.g. ["FY25"]
    "SCENARIO": None,      # e.g. ["PLAN", "FORECAST"]
    "VERSION": None,       # e.g. ["Final"]
    "ENTITY": None         # optional, add more keys as needed
}

def apply_filters(df):
    conds = []
    for col, vals in filter_dict.items():
        if vals:  # only apply if a list of filter values is provided
            conds.append(F.col(col).isin(vals))
    if conds:
        return df.filter(reduce(lambda a,b: a & b, conds))
    return df

# -----------------------------
# LOAD FACT TABLES
# -----------------------------
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")

# Apply front-end filters (currently no filters active)
actual_df   = apply_filters(actual_df)
plan_df     = apply_filters(plan_df)
forecast_df = apply_filters(forecast_df)

# -----------------------------
# DEDUPE LOGIC
# -----------------------------
def find_amount_col(df):
    for c in df.columns:
        if c.lower() == "amount":
            return c
    raise ValueError("No Amount column found (case-insensitive) in dataframe.")

def dedupe_sum_amount(df):
    amt_col = find_amount_col(df)
    key_cols = [c for c in df.columns if c != amt_col]
    return df.groupBy(key_cols).agg(F.sum(F.col(amt_col)).alias(amt_col))

# Deduplicate each fact
actual_d   = dedupe_sum_amount(actual_df)
plan_d     = dedupe_sum_amount(plan_df)
forecast_d = dedupe_sum_amount(forecast_df)

# Add FACT_SOURCE column if desired
if ADD_FACT_SOURCE:
    actual_d   = actual_d.withColumn("FACT_SOURCE", F.lit("ACTUALS"))
    plan_d     = plan_d.withColumn("FACT_SOURCE", F.lit("PLAN"))
    forecast_d = forecast_d.withColumn("FACT_SOURCE", F.lit("FORECAST"))

# -----------------------------
# ALIGN SCHEMAS BY COLUMN NAME
# -----------------------------
def align_to_columns(df, target_cols):
    df_cols = set(df.columns)
    return df.select(
        *[F.col(c) if c in df_cols else F.lit(None).alias(c) for c in target_cols]
    )

master_cols = actual_d.columns
plan_d_aligned     = align_to_columns(plan_d, master_cols)
forecast_d_aligned = align_to_columns(forecast_d, master_cols)

# -----------------------------
# CONCATENATE (STACK ALL)
# -----------------------------
facts_concat = (
    actual_d
      .unionByName(plan_d_aligned, allowMissingColumns=True)
      .unionByName(forecast_d_aligned, allowMissingColumns=True)
)

# -----------------------------
# SAVE TO FILES → SQL_DUMPS
# -----------------------------
writer = facts_concat
if COALESCE_SINGLE_FILE and OUTPUT_FORMAT in ("parquet","csv"):
    writer = writer.coalesce(1)

writer = writer.write.mode("overwrite")

if OUTPUT_FORMAT == "csv":
    writer.format("csv").option("header","true").save(TARGET_PATH)
elif OUTPUT_FORMAT == "delta":
    writer.format("delta").option("overwriteSchema","true").save(TARGET_PATH)
else:  # parquet (default)
    writer.format("parquet").save(TARGET_PATH)

print(f"[OK] Unified, deduped, and filter-ready dataset saved at: {TARGET_PATH}")
