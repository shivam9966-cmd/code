# ===================================================
# Granular FY25 KPI Dataset (Revenue, OPEX, EBITDA)
# Adds Actual/Forecasted columns (from SCENARIO)
# Uses full column names (Account_L4, Cost_Center, etc.)
# Saves as single CSV file (overwrite)
# ===================================================

import pyspark.sql.functions as F

# -----------------------------
# 1️⃣ Load tables
# -----------------------------
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")

account_df    = spark.table("FPNA_FISRPT_SILVER.dbo.account")
product_df    = spark.table("FPNA_FISRPT_SILVER.dbo.product")
costcenter_df = spark.table("FPNA_FISRPT_SILVER.dbo.cost_center")

# -----------------------------
# 2️⃣ Filter only FY25 early (performance gain)
# -----------------------------
actual_df   = actual_df.filter(F.col("YEAR") == "FY25")
plan_df     = plan_df.filter(F.col("YEAR") == "FY25")
forecast_df = forecast_df.filter(F.col("YEAR") == "FY25")

# -----------------------------
# 3️⃣ Function: join dims, tag KPIs, add FACT_SOURCE
# -----------------------------
def prepare_fact(df, source_name):
    d  = df.alias("d")
    ad = account_df.alias("ad")
    pd = product_df.alias("pd")
    cc = costcenter_df.alias("cc")

    # Join with dimension tables
    joined = (
        d.join(ad, F.col("d.ACCOUNT") == F.col("ad.Account"), "left")
         .join(pd, F.col("d.PRODUCTLINE") == F.col("pd.ProductLine"), "left")
         .join(cc, F.col("d.COSTCENTER") == F.col("cc.COSTCENTER"), "left")
    )

    # Keep only Revenue, OPEX, EBITDA
    joined = joined.withColumn(
        "KPI",
        F.when(F.col("ad.L5_Description") == "Revenue", "Revenue")
         .when(F.col("ad.L5_Description") == "Operating Expenses", "OPEX")
         .when(F.col("ad.L4") == "EBITDA", "EBITDA")
    ).filter(F.col("KPI").isNotNull())

    # Rename key hierarchy columns for readability
    renamed = joined.select(
        *[F.col("d." + c).alias(c) for c in df.columns],  # all fact columns
        F.col("ad.L4").alias("Account_L4"),
        F.col("ad.L5_Description").alias("Account_L5_Description"),
        F.col("pd.ProductLine").alias("Product_Line"),
        F.col("pd.L3_Description").alias("Product_L3_Description"),
        F.col("cc.COSTCENTER").alias("Cost_Center"),
        F.col("cc.L3_Description").alias("Cost_Center_L3_Description"),
        F.col("KPI"),
        F.lit(source_name).alias("FACT_SOURCE")
    )

    return renamed

# -----------------------------
# 4️⃣ Apply to all fact tables
# -----------------------------
act_kpi  = prepare_fact(actual_df, "ACTUALS")
plan_kpi = prepare_fact(plan_df, "PLAN")
fcst_kpi = prepare_fact(forecast_df, "FORECAST")

# -----------------------------
# 5️⃣ Union all together
# -----------------------------
granular = (
    act_kpi
    .unionByName(plan_kpi, allowMissingColumns=True)
    .unionByName(fcst_kpi, allowMissingColumns=True)
)

# -----------------------------
# 6️⃣ Add Actual / Forecasted columns (from SCENARIO)
# -----------------------------
granular = (
    granular
    .withColumn(
        "Actual",
        F.when(
            (F.col("FACT_SOURCE") == "FORECAST") &
            (F.col("SCENARIO").rlike("^F\\d+_\\d+$")),
            F.regexp_extract("SCENARIO", "F(\\d+)_\\d+", 1)
        )
    )
    .withColumn(
        "Forecasted",
        F.when(
            (F.col("FACT_SOURCE") == "FORECAST") &
            (F.col("SCENARIO").rlike("^F\\d+_\\d+$")),
            F.regexp_extract("SCENARIO", "F\\d+_(\\d+)", 1)
        )
    )
)

# -----------------------------
# 7️⃣ Save as single CSV (overwrite)
# -----------------------------
TARGET_PATH = "Files/SQL_DUMPS/FACTS_CONCAT_DEDUP_JOINED_KPI"

(
    granular
      .coalesce(1)                     # one CSV file
      .write
      .mode("overwrite")               # overwrite existing folder
      .option("header", "true")        # add headers
      .csv(TARGET_PATH)
)

print(f"[✅] FY25 granular CSV created successfully at → {TARGET_PATH}")
