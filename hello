from pyspark.sql import functions as F, Window

# --- Config ---
ACCOUNT_COL = "ACCOUNT"
TARGET = 5000   # feel free to change (total can end up < or > this)

# 1) Counts per account, ascending
acct_counts = (granular
               .groupBy(ACCOUNT_COL)
               .count()
               .orderBy(F.col("count").asc(), F.col(ACCOUNT_COL).asc()))

# 2) Cumulative sum over ascending accounts
w = Window.orderBy(F.col("count").asc(), F.col(ACCOUNT_COL).asc())
acct_counts = (acct_counts
               .withColumn("row_number", F.row_number().over(w))
               .withColumn("cumulative_sum", F.sum("count").over(w)))

# 3) Include all accounts up to TARGET, plus the first account that pushes past it (if any)
exceed = (acct_counts
          .filter(F.col("cumulative_sum") > TARGET)
          .orderBy("cumulative_sum")
          .select("row_number")
          .limit(1)
          .collect())

if exceed:
    last_rownum = exceed[0][0]
    included_accounts = (acct_counts
                         .where((F.col("cumulative_sum") <= TARGET) | (F.col("row_number") == last_rownum))
                         .select(ACCOUNT_COL).distinct())
else:
    # never exceeded -> include all accounts
    included_accounts = acct_counts.select(ACCOUNT_COL).distinct()

# 4) Build the subset: this ensures ALL rows for any included account are present
subset = granular.join(included_accounts, on=ACCOUNT_COL, how="inner")

# 5) Quick sanity: show coverage and a small preview
print(f"âœ… Total rows in subset (full accounts only): {subset.count():,}")
(acct_counts.join(included_accounts, on=ACCOUNT_COL, how="inner")
            .orderBy(F.col("count").asc(), F.col(ACCOUNT_COL).asc())
            .select(ACCOUNT_COL, "count", "cumulative_sum")
            .show(30, truncate=False))

subset.show(20, truncate=False)  # or display(subset.limit(20)) in notebooks
