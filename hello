
import pyspark.sql.functions as F
from functools import reduce

# -----------------------------
# CONFIG
# -----------------------------
TARGET_PATH = "/lakehouse/default/Files/SQL_DUMPS/FACTS_CONCAT_DEDUP_JOINED_KPI"
OUTPUT_FORMAT = "parquet"          # "parquet" | "delta" | "csv"
COALESCE_SINGLE_FILE = True        # single file output
APPLY_KPI_FILTER = True            # keep only Revenue / OpEx / EBITDA
EBITDA_CODES = []                  # optional explicit account codes, e.g. ["499999","EBITDA_TOTAL"]

# Optional front-end style filters (leave None to include all)
filter_dict = {
    "YEAR": None,        # e.g. ["FY25"]
    "SCENARIO": None,    # e.g. ["PLAN","FORECAST","ACTUALS"]
    "VERSION": None,     # e.g. ["Final"]
    # add more like "ENTITY": ["India"] if needed
}

def apply_filters(df):
    conds = []
    for col, vals in filter_dict.items():
        if vals:
            conds.append(F.col(col).isin(vals))
    return df.filter(reduce(lambda a,b: a & b, conds)) if conds else df

# -----------------------------
# LOAD TABLES
# -----------------------------
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")

account_df    = spark.table("FPNA_FISRPT_SILVER.dbo.account")      # keys/levels: Account, L*, L*_Description
product_df    = spark.table("FPNA_FISRPT_SILVER.dbo.product")      # key: ProductLine
costcenter_df = spark.table("FPNA_FISRPT_SILVER.dbo.cost_center")  # key column is COSTCENTER (as you said)

# Apply front-end filters if any
actual_df   = apply_filters(actual_df)
plan_df     = apply_filters(plan_df)
forecast_df = apply_filters(forecast_df)

# -----------------------------
# DEDUPE PER FACT (sum Amount when only Amount differs)
# -----------------------------
def find_amount_col(df):
    for c in df.columns:
        if c.lower() == "amount":
            return c
    raise ValueError("No 'Amount' column found (case-insensitive).")

def dedupe_sum_amount(df):
    amt = find_amount_col(df)
    keys = [c for c in df.columns if c != amt]
    return df.groupBy(keys).agg(F.sum(F.col(amt)).alias(amt))

actual_d    = dedupe_sum_amount(actual_df).withColumn("FACT_SOURCE", F.lit("ACTUALS"))
plan_d      = dedupe_sum_amount(plan_df).withColumn("FACT_SOURCE", F.lit("PLAN"))
forecast_d  = dedupe_sum_amount(forecast_df).withColumn("FACT_SOURCE", F.lit("FORECAST"))

# -----------------------------
# ALIGN BY COLUMN NAME & CONCAT
# -----------------------------
def align_to_columns(df, target_cols):
    s = set(df.columns)
    return df.select(*[F.col(c) if c in s else F.lit(None).alias(c) for c in target_cols])

master_cols = actual_d.columns
plan_d_aligned     = align_to_columns(plan_d, master_cols)
forecast_d_aligned = align_to_columns(forecast_d, master_cols)

facts_concat = (
    actual_d
      .unionByName(plan_d_aligned, allowMissingColumns=True)
      .unionByName(forecast_d_aligned, allowMissingColumns=True)
).alias("f")

# -----------------------------
# JOIN ONCE TO DIMENSIONS (correct keys)
# facts.ACCOUNT      = account.Account
# facts.PRODUCTLINE  = product.ProductLine
# facts.COSTCENTER   = cost_center.COSTCENTER    (per your note)
# -----------------------------
ad = account_df.alias("ad")
pd = product_df.alias("pd")
cc = costcenter_df.alias("cc")

joined = (
    facts_concat
      .join(ad, F.col("f.ACCOUNT")     == F.col("ad.Account"),     "left")
      .join(pd, F.col("f.PRODUCTLINE") == F.col("pd.ProductLine"), "left")
      .join(cc, F.col("f.COSTCENTER")  == F.col("cc.COSTCENTER"),  "left")
)

# -----------------------------
# SELECT: keep fact cols; prefix dim cols to avoid name clashes
# -----------------------------
fact_cols = [F.col(f"f.{c}").alias(c) for c in facts_concat.columns]
acc_cols  = [F.col(f"ad.{c}").alias(f"acc_{c}")  for c in account_df.columns]
prod_cols = [F.col(f"pd.{c}").alias(f"prod_{c}") for c in product_df.columns]
cc_cols   = [F.col(f"cc.{c}").alias(f"cc_{c}")   for c in costcenter_df.columns]

final_df = joined.select(*(fact_cols + acc_cols + prod_cols + cc_cols))

# -----------------------------
# OPTIONAL: keep only Revenue / OpEx / EBITDA (as in your SS)
# Uses account hierarchy: acc_L5_Description (Revenue/Operating Expenses), acc_L4 (EBITDA)
# -----------------------------
if APPLY_KPI_FILTER:
    has_l5_desc = "acc_L5_Description" in final_df.columns
    has_l4      = "acc_L4" in final_df.columns
    has_acc     = "acc_Account" in final_df.columns

    revenue_expr = F.col("acc_L5_Description") == "Revenue" if has_l5_desc else F.lit(False)
    opex_expr    = F.col("acc_L5_Description") == "Operating Expenses" if has_l5_desc else F.lit(False)
    ebitda_expr  = (F.col("acc_L4") == "EBITDA") if has_l4 else F.lit(False)
    if has_acc and EBITDA_CODES:
        ebitda_expr = ebitda_expr | F.col("acc_Account").isin(EBITDA_CODES)

    final_df = (
        final_df
          .withColumn(
              "KPI",
              F.when(revenue_expr, "Revenue")
               .when(opex_expr, "Operating Expenses")
               .when(ebitda_expr, "EBITDA")
          )
          .filter(F.col("KPI").isNotNull())
    )

# -----------------------------
# SAVE TO FILES â†’ SQL_DUMPS
# -----------------------------
writer = final_df
if COALESCE_SINGLE_FILE and OUTPUT_FORMAT in ("parquet","csv"):
    writer = writer.coalesce(1)

w = writer.write.mode("overwrite")
if OUTPUT_FORMAT == "csv":
    w.format("csv").option("header","true").save(TARGET_PATH)
elif OUTPUT_FORMAT == "delta":
    w.format("delta").option("overwriteSchema","true").save(TARGET_PATH)
else:  # parquet
    w.format("parquet").save(TARGET_PATH)

print(f"[OK] Hierarchy-enabled KPI file saved at: {TARGET_PATH}  (format={OUTPUT_FORMAT})")
