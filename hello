from pyspark.sql import functions as F, Window as W

def detect_anomalies_dynamic_threshold(
    df,
    group_cols,                 # e.g., ['org','country','state','dc','product','sku']
    date_col,                   # e.g., 'date' / 'txn_date'
    metric_col,                 # e.g., 'amount' / 'revenue'
    metric_name,                # e.g., 'Revenue' (used in `aan`)
    method="mad",               # 'mad' (robust) or 'std' (volatility)
    hist_window=14,             # lookback points for dynamic band
    k=3.0,                      # band width multiplier
    min_points=7,               # min history points required
    level_tag=None              # optional label like 'org-**-sku'
):
    """
    Returns a DataFrame with the anomaly report schema:
      aan | aan_date | predictive_date | aan_level | aan_delta | value | expected | lower | upper | method
    """

    
    hist_w = (
        W.partitionBy(*group_cols)
         .orderBy(F.col(date_col))
         .rowsBetween(-hist_window, -1)
    )

    
    median_hist = F.percentile_approx(F.col(metric_col), 0.5, 2048).over(hist_w)
    abs_dev     = F.abs(F.col(metric_col) - median_hist)
    mad_hist    = F.percentile_approx(abs_dev, 0.5, 2048).over(hist_w)

    
    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)

    expected = F.when(F.lower(F.lit(method))=="mad", median_hist).otherwise(mean_hist)

    
    floor = F.lit(1e-9)
    band_half = F.when(
        F.lower(F.lit(method))=="mad",
        F.coalesce(mad_hist, F.lit(0.0)) * F.lit(1.4826)
    ).otherwise(
        F.coalesce(std_hist, F.lit(0.0))
    )
    band_half = F.greatest(band_half, floor) * F.lit(k)

    lower = expected - band_half
    upper = expected + band_half

    
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

   
    aan_delta = F.when(
        expected.isNotNull() & (expected != 0),
        (F.col(metric_col) - expected) / F.abs(expected) * F.lit(100.0)
    ).otherwise(F.lit(None).cast("double"))

    
    group_path = F.concat_ws(">", *[F.coalesce(F.col(c).cast("string"), F.lit("âˆ…")) for c in group_cols])
    aan = F.concat_ws("::", F.lit(metric_name), group_path)
    aan_level = F.lit(level_tag) if level_tag else F.concat_ws(" ", F.lit(f"lvl-{len(group_cols)}"), F.lit("("), group_path, F.lit(")"))

    out = (
        df
        .withColumn("_expected", expected)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("_method", F.lower(F.lit(method)))
        .withColumn("aan_delta", F.round(aan_delta, 3))
        .withColumn("aan", aan)
        .withColumn("aan_level", aan_level)
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn(
            "is_anomaly",
            F.when(
                (F.col("_cnt_hist") >= F.lit(min_points)) &
                ((F.col(metric_col) < F.col("_lower")) | (F.col(metric_col) > F.col("_upper"))),
                F.lit(1)
            ).otherwise(F.lit(0))
        )
    )

    return (
        out
        .filter(F.col("is_anomaly") == 1)
        .select(
            "aan",
            "aan_date",
            "predictive_date",
            "aan_level",
            "aan_delta",
            F.col(metric_col).alias("value"),
            F.col("_expected").alias("expected"),
            F.col("_lower").alias("lower"),
            F.col("_upper").alias("upper"),
            F.col("_method").alias("method")
        )
    )
