from pyspark.sql import functions as F

# Ensure DATE column is proper date
granular_df = granular_df.withColumn("DATE", F.to_date("DATE"))

# Filter FY25 + PLAN + FINAL + Recurring Revenue
fy25_plan_final_rec_rev = granular_df.filter(
    (F.col("YEAR") == "FY25") &
    (F.col("SCENARIO") == "PLAN") &
    (F.col("VERSION") == "Final") &
    (F.col("Account_L6_Description") == "Recurring Revenue")
)

# Sum of recurring revenue
fy25_plan_final_rec_rev_sum = fy25_plan_final_rec_rev.agg(
    F.sum("Amount").alias("Recurring_Revenue_SUM")
)

fy25_plan_final_rec_rev_sum.show(truncate=False)
