
BASE_DIR     = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
OUTPUT_DELTA = f"{BASE_DIR}/anomaly_report_rev_opex_ebitda_delta"

PRODUCT_HIER = [
    "product_L3_Description","product_L4_Description","product_L5_Description",
    "product_L6_Description","product_L7_Description","product_L8_Description"
]
ACCOUNT_HIER = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCNTR_HIER= ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

REQUIRED_COLS = PRODUCT_HIER + ACCOUNT_HIER + COSTCNTR_HIER + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION",
    "Amount","YEAR","QUARTER"
]


from pyspark.sql import functions as F

q_start_month = F.when(F.col("QUARTER")==1, F.lit(1)) \
                 .when(F.col("QUARTER")==2, F.lit(4)) \
                 .when(F.col("QUARTER")==3, F.lit(7)) \
                 .otherwise(F.lit(10))


raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

df = raw_df.select([c for c in REQUIRED_COLS if c in raw_df.columns])

df = df.withColumn("pred_date",
                   F.to_date(F.concat_ws("-",
                                         F.col("YEAR").cast("int"),
                                         q_start_month,
                                         F.lit(1)), "yyyy-M-d"))


HIERARCHIES = [ACCOUNT_HIER, PRODUCT_HIER, COSTCNTR_HIER]


def run_kpi(label):
    kpi_df = df.filter(F.col("account_L5_Description") == F.lit(label))
    return detect_anomalies_mavg_std_report(
        df        = kpi_df,
        hierarchies = HIERARCHIES,
        date_col  = "pred_date",
        metric_col= "Amount",
        metric_name = label,
        hist_window = 14,   # tune as needed
        k           = 3.0,
        min_points  = 7,
    )

rev_anoms    = run_kpi("Revenue")
opex_anoms   = run_kpi("OPEX")
ebitda_anoms = run_kpi("EBITDA")


from functools import reduce
from pyspark.sql import DataFrame
combined_anoms = reduce(DataFrame.unionByName, [rev_anoms, opex_anoms, ebitda_anoms])

(combined_anoms
 .write
 .mode("overwrite")
 .format("delta")
 .save(OUTPUT_DELTA))

display(combined_anoms.orderBy(F.col("predictive_date").desc()).limit(50))
print(f"Saved combined anomaly report to: {OUTPUT_DELTA}")
print(f"Total anomalies across Revenue/OPEX/EBITDA: {combined_anoms.count()}")
