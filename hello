from pyspark.sql import functions as F

table_name = "your_table_name"

# 1. Create SQL insert string column
df_sql = df_final.select(
    F.concat(
        F.lit(f"INSERT INTO {table_name} ("),
        F.lit(",".join(df_final.columns)),
        F.lit(") VALUES ("),
        F.concat_ws("','", *[F.coalesce(F.col(c).cast("string"), F.lit("")) for c in df_final.columns]),
        F.lit("');")
    ).alias("sql_line")
)

# 2. Write as single text file
output_path = "Files/SQL_DUMPS/FINAL_EXPORTS/final_dump"
(
    df_sql.coalesce(1)                  # <--- forces 1 output file
          .write.mode("overwrite")
          .text(output_path)
)

print("SQL dump created at:", output_path)
