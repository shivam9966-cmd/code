
from pyspark.sql import functions as F, Window as W
from functools import reduce
from pyspark.sql import DataFrame

BASE_DIR    = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
HIST_WINDOW = 14
K_FACTOR    = 3.0
MIN_POINTS  = 7

PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

REQUIRED_COLS = PRODUCT_LEVELS + ACCOUNT_LEVELS + COSTCENTER_LEVELS + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION","Amount","YEAR","QUARTER"
]


raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

df = raw_df.select([c for c in REQUIRED_COLS if c in raw_df.columns])

q_start_month = (F.when(F.col("QUARTER").cast("int")==1, F.lit(1))
                 .when(F.col("QUARTER").cast("int")==2, F.lit(4))
                 .when(F.col("QUARTER").cast("int")==3, F.lit(7))
                 .otherwise(F.lit(10)))

df = df.withColumn(
    "pred_date",
    F.to_date(F.concat_ws("-", F.col("YEAR").cast("int"), q_start_month, F.lit(1)), "yyyy-M-d")
)


def norm_lower(colname): 
    return F.trim(F.lower(F.col(colname)))

ACCOUNT_L5_COL = next((c for c in df.columns if c.lower() == "account_l5_description"), None)

def kpi_slice(df_in, label):
    x = norm_lower(ACCOUNT_L5_COL)
    if label == "OPEX":
        return df_in.filter(x.rlike(r"(operating expense(s)?|opex)"))
    elif label == "EBITDA":
        return df_in.filter(x.rlike(r"ebitda"))
    else:
        return df_in.filter(x.rlike(r"revenue"))

# =========================
# 4️⃣ ANOMALY DETECTOR FUNCTION
# =========================
def detect_anomalies_mavg_std_report(
    df_in,
    account_levels,
    product_levels,
    costcenter_levels,
    date_col,
    metric_col,
    metric_name,
    hist_window=14,
    k=3.0,
    min_points=7,
):
    # helper: non-null concat
    def join_non_null(cols):
        return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])
    def join_level_names(cols):
        return F.concat_ws("-*-", *[F.lit(c.replace("_Description","")) for c in cols])
    def deepest_present(cols):
        expr = None
        for c in reversed(cols):
            expr = F.when(F.col(c).isNotNull(), F.col(c).cast("string")).otherwise(expr)
        return expr

    acct_vals, prod_vals, cc_vals = join_non_null(account_levels), join_non_null(product_levels), join_non_null(costcenter_levels)
    acct_names, prod_names, cc_names = join_level_names(account_levels), join_level_names(product_levels), join_level_names(costcenter_levels)

    aan_level_expr = F.concat_ws(
        "-**-",
        F.concat_ws("-*-", F.lit("Account"), acct_names),
        F.concat_ws("-*-", F.lit("ProductLine"), prod_names),
        F.concat_ws("-*-", F.lit("CostCenter"), cc_names)
    )

    aan_value_expr = F.concat_ws("-**-", acct_vals, prod_vals, cc_vals)
    group_cols = account_levels + product_levels + costcenter_levels
    hist_w = (W.partitionBy(*group_cols).orderBy(F.col(date_col)).rowsBetween(-hist_window, -1))

    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    lower     = mean_hist - F.lit(k)*std_hist
    upper     = mean_hist + F.lit(k)*std_hist
    cnt_hist  = F.count(F.col(metric_col)).over(hist_w)

    delta = F.when(mean_hist != 0, (F.col(metric_col)-mean_hist)/F.abs(mean_hist)*100)

    aan_key = F.concat_ws("::", F.lit(metric_name), aan_value_expr)
    acct_deep, prod_deep, cc_deep = deepest_present(account_levels), deepest_present(product_levels), deepest_present(costcenter_levels)

    acct_part = F.when(acct_deep.isNotNull(), F.concat(F.lit("Account="), acct_deep))
    prod_part = F.when(prod_deep.isNotNull(), F.concat(F.lit("ProductLine="), prod_deep))
    cc_part   = F.when(cc_deep.isNotNull(),   F.concat(F.lit("CostCenter="), cc_deep))

    out = (df_in
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_delta", F.round(delta, 3))
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("aan_level", aan_level_expr)
        .withColumn("aan_value", aan_value_expr)
        .withColumn("aan_name", F.lit(metric_name))
        .withColumn("aan", aan_key)
        .withColumn("is_anomaly", ((F.col(metric_col) < lower) | (F.col(metric_col) > upper)) & (cnt_hist >= F.lit(min_points)))
    )

    rank_w = W.partitionBy("predictive_date", "aan_name").orderBy(F.desc(F.abs(F.col("_delta"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    inference_expr = F.concat_ws(
        " ",
        F.lit(metric_name), F.lit("anomaly: deviation"),
        F.format_string("%.1f%%", F.col("_delta")),
        F.lit("at"),
        F.concat_ws(", ", acct_part, prod_part, cc_part),
        F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
        F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
        F.lit("(band:"), F.format_string("%.2f", F.col("_lower")), F.lit("to"),
        F.format_string("%.2f", F.col("_upper")), F.lit(")")
    )

    result = (out.filter(F.col("is_anomaly"))
        .select(
            "aan","aan_date","predictive_date",
            "aan_level",F.format_string("%.3f", F.col("_delta")).alias("aan_delta"),
            F.col("_expected").alias("aan_attr"),
            F.col("_rank").cast("string").alias("aan_ranking"),
            inference_expr.alias("inference"),
            "aan_name","aan_value",
            F.concat_ws("|",
                F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                F.col("aan_name"),F.col("aan")).alias("anomaly_id"),
            F.lit("Point Anomaly").alias("anomaly_type"),
            F.lit(False).alias("rca_present"),
            F.lit(False).alias("impact_present")
        )
    )
    return result

# =========================
# 5️⃣ GENERATE ANOMALIES
# =========================
def run_kpi(label):
    kpi_df = kpi_slice(df, label)
    return detect_anomalies_mavg_std_report(
        df_in=kpi_df,
        account_levels=ACCOUNT_LEVELS,
        product_levels=PRODUCT_LEVELS,
        costcenter_levels=COSTCENTER_LEVELS,
        date_col="pred_date",
        metric_col="Amount",
        metric_name=label,
        hist_window=HIST_WINDOW,
        k=K_FACTOR,
        min_points=MIN_POINTS
    )

ebitda_anoms  = run_kpi("EBITDA")
opex_anoms    = run_kpi("OPEX")
revenue_anoms = run_kpi("Revenue")

anomalies_df = reduce(DataFrame.unionByName, [ebitda_anoms, opex_anoms, revenue_anoms])

display(anomalies_df.orderBy(F.col("predictive_date").desc()).limit(100))
print("Counts → EBITDA:", ebitda_anoms.count(), "OPEX:", opex_anoms.count(), "Revenue:", revenue_anoms.count())

# =========================
# 6️⃣ VALIDATE ANOMALIES
# =========================
GROUP_COLS = ACCOUNT_LEVELS + PRODUCT_LEVELS + COSTCENTER_LEVELS
KPIS = ["EBITDA","OPEX","Revenue"]

def _value_block(cols): return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])
def _full_value_path(): return F.concat_ws("-**-", _value_block(ACCOUNT_LEVELS), _value_block(PRODUCT_LEVELS), _value_block(COSTCENTER_LEVELS))
def _aan_key(metric_name_lit): return F.concat_ws("::", metric_name_lit, _full_value_path())

def _recompute_for_kpi(df_base, kpi_label):
    base = kpi_slice(df_base, kpi_label).withColumn("aan_key", _aan_key(F.lit(kpi_label)))
    hist_w = (W.partitionBy(*GROUP_COLS).orderBy("pred_date").rowsBetween(-HIST_WINDOW, -1))
    base = (base
        .withColumn("rolling_mean", F.avg("Amount").over(hist_w))
        .withColumn("rolling_std",  F.stddev_samp("Amount").over(hist_w))
        .withColumn("lower_band",   F.col("rolling_mean") - F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("upper_band",   F.col("rolling_mean") + F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("recomp_delta",
            F.when((F.col("rolling_mean").isNotNull()) & (F.col("rolling_mean") != 0),
                   (F.col("Amount")-F.col("rolling_mean"))/F.abs(F.col("rolling_mean"))*100.0))
        .select("aan_key","pred_date","Amount","rolling_mean","rolling_std","lower_band","upper_band","recomp_delta")
        .withColumn("aan_name", F.lit(kpi_label))
    )
    return base

recomputed_all = reduce(DataFrame.unionByName, [_recompute_for_kpi(df, k) for k in KPIS])

# --- join & check ---
anoms = (anomalies_df
         .withColumn("aan_delta_dbl", F.col("aan_delta").cast("double"))
         .select("aan","aan_name","predictive_date","aan_delta_dbl","aan_attr",
                 "aan_level","aan_value","inference","anomaly_id"))

joined = (anoms.alias("a")
          .join(recomputed_all.alias("r"),
                (F.col("a.aan") == F.col("r.aan_key")) &
                (F.col("a.predictive_date") == F.col("r.pred_date")) &
                (F.col("a.aan_name") == F.col("r.aan_name")),
                "left"))

EPS_DELTA_PP = 0.05
EPS_MEAN     = 1e-6

validated = (joined
    .withColumn("delta_diff_pp", F.round(F.col("r.recomp_delta") - F.col("a.aan_delta_dbl"), 4))
    .withColumn("expected_diff", F.round(F.col("r.rolling_mean") - F.col("a.aan_attr"), 6))
    .withColumn("is_outside_band", (F.col("r.Amount") < F.col("r.lower_band")) | (F.col("r.Amount") > F.col("r.upper_band")))
    .withColumn("delta_ok", F.abs(F.col("delta_diff_pp")) <= F.lit(EPS_DELTA_PP))
    .withColumn("expected_ok", F.abs(F.col("expected_diff")) <= F.lit(EPS_MEAN))
    .withColumn("band_ok", F.col("is_outside_band"))
    .withColumn("overall_ok", F.col("delta_ok") & F.col("expected_ok") & F.col("band_ok"))
)

summary = (validated.groupBy("a.aan_name")
           .agg(F.count("*").alias("anomaly_rows"),
                F.sum(F.when(F.col("overall_ok"), 1).otherwise(0)).alias("verified_rows"),
                (F.sum(F.when(F.col("overall_ok"), 1).otherwise(0))/F.count("*")*100).alias("verified_pct"))
           .orderBy("a.aan_name"))
display(summary)

issues = (validated.filter(~F.col("overall_ok"))
          .select(
              F.col("a.aan_name").alias("kpi"),
              F.col("a.predictive_date").alias("date"),
              "a.aan_level","a.aan_value",
              F.col("r.Amount").alias("actual"),
              F.col("r.rolling_mean").alias("recomp_baseline"),
              F.col("r.lower_band").alias("recomp_lower"),
              F.col("r.upper_band").alias("recomp_upper"),
              F.col("a.aan_attr").alias("reported_baseline"),
              F.col("a.aan_delta_dbl").alias("reported_delta_pct"),
              F.col("r.recomp_delta").alias("recomp_delta_pct"),
              "delta_diff_pp","expected_diff","is_outside_band",
              "delta_ok","expected_ok","band_ok",
              "a.inference","a.anomaly_id"
          )
          .orderBy(F.col("date").desc(), F.abs(F.col("delta_diff_pp")).desc()))
display(issues.limit(200))
