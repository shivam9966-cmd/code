# =========================
# Imports
# =========================
import pyspark.sql.functions as F
from functools import reduce  # <-- use Python's reduce for combining conditions

# =========================
# Load facts & dimensions
# =========================
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")

costcenter_df = spark.table("FPNA_FISRPT_SILVER.dbo.cost_center")
product_df    = spark.table("FPNA_FISRPT_SILVER.dbo.product")
account_df    = spark.table("FPNA_FISRPT_SILVER.dbo.account")

# =========================
# Helpers
# =========================
def cols_except(df, exclude):
    ex = set(exclude)
    return [c for c in df.columns if c not in ex]

# ---------- A) SURFACE DUPLICATES (no mutation) ----------
def find_duplicates(df, fact_name):
    """
    Returns:
      exact_dups: rows where *all* columns match (count > 1)
      amount_only: groups where all non-Amount columns are same but Amount differs
    """
    # 1) Exact duplicate rows
    exact_dups = (
        df.groupBy(df.columns)
          .count()
          .filter(F.col("count") > 1)
          .withColumn("FACT_SOURCE", F.lit(fact_name))
    )

    # 2) Only-Amount-diff detection
    key_cols = cols_except(df, ["Amount"])
    amount_only = (
        df.groupBy(key_cols)
          .agg(
              F.count(F.lit(1)).alias("row_count"),
              F.countDistinct("Amount").alias("amount_variants"),
              F.sum("Amount").alias("summed_amount_for_key")
          )
          .filter(F.col("amount_variants") > 1)
          .withColumn("FACT_SOURCE", F.lit(fact_name))
    )
    return exact_dups, amount_only

# Run duplicate surfacing (review these views before proceeding if you like)
actual_exact_dups,   actual_amount_only   = find_duplicates(actual_df,   "ACTUALS")
plan_exact_dups,     plan_amount_only     = find_duplicates(plan_df,     "PLAN")
forecast_exact_dups, forecast_amount_only = find_duplicates(forecast_df, "FORECAST")

actual_exact_dups.createOrReplaceTempView("vw_exact_dups_actuals")
plan_exact_dups.createOrReplaceTempView("vw_exact_dups_plan")
forecast_exact_dups.createOrReplaceTempView("vw_exact_dups_forecast")

actual_amount_only.createOrReplaceTempView("vw_amount_only_dups_actuals")
plan_amount_only.createOrReplaceTempView("vw_amount_only_dups_plan")
forecast_amount_only.createOrReplaceTempView("vw_amount_only_dups_forecast")

# Example to inspect:
# spark.sql("SELECT * FROM vw_amount_only_dups_plan ORDER BY row_count DESC").show(50, False)

# ---------- B) CANONICAL DEDUPE (sum Amount when only Amount differs) ----------
def dedupe_sum_amount(df):
    key_cols = cols_except(df, ["Amount"])
    return df.groupBy(key_cols).agg(F.sum("Amount").alias("Amount"))

# ---------- C) ENRICH + KPI FILTER + FRONT-END FILTERS ----------
def build_granular(fact_df, fact_name, scenarios=None, years=None, versions=None):
    """
    - Dedupes (sum Amount where only Amount differs)
    - Joins to dims (left)
    - Filters by SCENARIO/YEAR/VERSION (lists supported)
    - Keeps KPIs: Revenue, Operating Expenses, EBITDA (from hierarchy)
    - Adds FACT_SOURCE tag
    """
    # 1) Dedup
    d = dedupe_sum_amount(fact_df).alias("d")

    # 2) Joins
    granular = (
        d.join(costcenter_df.alias("cc"), F.col("d.COSTCENTER")  == F.col("cc.CostCenter"),  "left")
         .join(product_df.alias("pd"),   F.col("d.PRODUCTLINE") == F.col("pd.ProductLine"), "left")
         .join(account_df.ali_
