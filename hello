
# Monthly anomalies (3 prior months) for Revenue / OPEX / EBITDA
# - Auto-pick latest Files/SQL_DUMPS/FINAL_EXPORTS/Run_*/granular_subset
# - Uses 'date' column (DateType); window = rowsBetween(-3, -1)
# - Dynamic threshold: mean Â± K * std
# - MIN_POINTS = 2 (monthly)
# - Outputs anomalies_df (no save)


from pyspark.sql import functions as F, Window as W
from pyspark.sql import DataFrame
from functools import reduce


BASE_PATH = "Files/SQL_DUMPS/FINAL_EXPORTS"

paths_df = (spark.read.format("binaryFile")
            .load(f"{BASE_PATH}/Run_*/granular_subset/*")
            .select("path"))
if paths_df.rdd.isEmpty():
    raise Exception("No files found under FINAL_EXPORTS/Run_*/granular_subset/*")

runs_df = paths_df.select(
    F.regexp_extract("path", r"(.*?/Run_[^/]+)/granular_subset/.*", 1).alias("run")
).distinct()
latest_run = runs_df.orderBy(F.col("run").desc()).limit(1).collect()[0]["run"]
DATA_PATH  = f"{latest_run}/granular_subset"
print(f"ðŸ“‚ Using latest dataset: {DATA_PATH}")


K_FACTOR    = 3.0
MIN_POINTS  = 2            # monthly â†’ need â‰¥2 past months
LOOKBACK_N  = 3            # use 3 PRIOR months in the window

PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]


df = (spark.read
      .format("csv")
      .option("header", True)
      .option("inferSchema", True)
      .option("recursiveFileLookup", "true")
      .load(DATA_PATH))


df = df.withColumn("date", F.to_date(F.col("date")))

# ---------- 4) KPI slicer (robust to label variations + column casing) ----------
def _choose_l5_col(dframe):
    for c in dframe.columns:
        if c.lower() == "account_l5_description":
            return c
    return "account_L5_Description"

ACCOUNT_L5_COL = _choose_l5_col(df)
def norm_lower(colname): return F.trim(F.lower(F.col(colname)))

def kpi_slice(df_in, label):
    x = norm_lower(ACCOUNT_L5_COL)
    if label == "OPEX":
        # matches "operating expense", "operating expenses", "opex"
        return df_in.filter(x.rlike(r"(?:^|\s)(operating expense(s)?|opex)(?:\s|$)"))
    elif label == "EBITDA":
        return df_in.filter(x.rlike(r"(?:^|\s)ebitda(?:\s|$)"))
    else:  # Revenue
        return df_in.filter(x.rlike(r"(?:^|\s)revenue(?:\s|$)"))


def detect_anomalies_mavg_std_report(
    df_in: DataFrame,
    account_levels,
    product_levels,
    costcenter_levels,
    date_col: str,        # "date"
    metric_col: str,      # "Amount"
    metric_name: str,     # "EBITDA" | "OPEX" | "Revenue"
    lookback_n: int = 3,  # prior months to use
    k: float = 3.0,
    min_points: int = 2,
) -> DataFrame:

    
    def join_non_null(cols):   return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])
    def join_level_names(cols):return F.concat_ws("-*-", *[F.lit(c.replace("_Description","")) for c in cols])
    def deepest_present(cols):
        expr=None
        for c in reversed(cols):
            expr = F.when(F.col(c).isNotNull(), F.col(c).cast("string")).otherwise(expr)
        return expr

   
    acct_vals, prod_vals, cc_vals    = map(join_non_null, [account_levels, product_levels, costcenter_levels])
    acct_names, prod_names, cc_names = map(join_level_names, [account_levels, product_levels, costcenter_levels])

    
    aan_level_expr = F.concat_ws(
        "-**-",
        F.concat_ws("-*-", F.lit("Account"),     acct_names),
        F.concat_ws("-*-", F.lit("ProductLine"), prod_names),
        F.concat_ws("-*-", F.lit("CostCenter"),  cc_names)
    )
    aan_value_expr = F.concat_ws("-**-", acct_vals, prod_vals, cc_vals)
    aan_key        = F.concat_ws("::", F.lit(metric_name), aan_value_expr)

    group_cols = account_levels + product_levels + costcenter_levels

    
    hist_w = (W.partitionBy(*group_cols)
                .orderBy(F.col(date_col))
                .rowsBetween(-lookback_n, -1))

    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    band_half = F.greatest(F.coalesce(std_hist, F.lit(0.0)), F.lit(1e-9)) * F.lit(k)
    lower, upper = mean_hist - band_half, mean_hist + band_half
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

    delta_num = F.when(
        mean_hist.isNotNull() & (mean_hist != 0),
        (F.col(metric_col) - mean_hist) / F.abs(mean_hist) * 100.0
    )

    # inference parts
    acct_deep, prod_deep, cc_deep = map(deepest_present, [account_levels, product_levels, costcenter_levels])
    acct_part = F.when(acct_deep.isNotNull(), F.concat(F.lit("Account="), acct_deep))
    prod_part = F.when(prod_deep.isNotNull(), F.concat(F.lit("ProductLine="), prod_deep))
    cc_part   = F.when(cc_deep.isNotNull(),   F.concat(F.lit("CostCenter="), cc_deep))

    out = (df_in
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("_delta_num", F.round(delta_num, 3))
        .withColumn("aan_level", aan_level_expr)
        .withColumn("aan_value", aan_value_expr)
        .withColumn("aan_name",  F.lit(metric_name))
        .withColumn("aan",       aan_key)
        .withColumn("is_anomaly",
            ((F.col(metric_col) < lower) | (F.col(metric_col) > upper)) &
            (cnt_hist >= F.lit(min_points)))
    )

    # rank by |delta|
    rank_w = W.partitionBy("predictive_date","aan_name").orderBy(F.desc(F.abs(F.col("_delta_num"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    inference_expr = F.concat_ws(
        " ",
        F.lit(metric_name), F.lit("anomaly: deviation"),
        F.format_string("%.1f%%", F.col("_delta_num")),
        F.lit("at"),
        F.concat_ws(", ", acct_part, prod_part, cc_part),
        F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
        F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
        F.lit("(band:"), F.format_string("%.2f", F.col("_lower")),
        F.lit("to"), F.format_string("%.2f", F.col("_upper")), F.lit(")")
    )

    return (out.filter(F.col("is_anomaly"))
        .select(
            "aan","aan_date","predictive_date",F.col(date_col).alias("date"),
            "aan_level",F.format_string("%.3f",F.col("_delta_num")).alias("aan_delta"),
            F.col("_expected").alias("aan_attr"),
            F.col("_rank").cast("string").alias("aan_ranking"),
            inference_expr.alias("inference"),
            "aan_name","aan_value",
            F.concat_ws("|",
                F.date_format(F.col("predictive_date"),"yyyy-MM-dd"),
                F.col("aan_name"),F.col("aan")).alias("anomaly_id"),
            F.lit("Point Anomaly").alias("anomaly_type"),
            F.lit(False).alias("rca_present"),
            F.lit(False).alias("impact_present")
        ))

# ---------- 6) Run for KPIs and union ----------
def run_kpi(label):
    kpi_df = kpi_slice(df, label)
    return detect_anomalies_mavg_std_report(
        df_in=kpi_df,
        account_levels=ACCOUNT_LEVELS,
        product_levels=PRODUCT_LEVELS,
        costcenter_levels=COSTCENTER_LEVELS,
        date_col="date",
        metric_col="Amount",
        metric_name=label,
        lookback_n=LOOKBACK_N,   # 3 prior months
        k=K_FACTOR,
        min_points=MIN_POINTS,
    )

ebitda_anoms  = run_kpi("EBITDA")
opex_anoms    = run_kpi("OPEX")
revenue_anoms = run_kpi("Revenue")

anomalies_df = reduce(DataFrame.unionByName, [ebitda_anoms, opex_anoms, revenue_anoms])

print("Counts â†’",
      "EBITDA:",  ebitda_anoms.count(),
      "OPEX:",    opex_anoms.count(),
      "Revenue:", revenue_anoms.count(),
      "All:",     anomalies_df.count())

display(anomalies_df.orderBy(F.col("predictive_date").desc()).limit(100))
