# Load your existing granular subset
df = spark.read.csv("/lakehouse/default/Files/granular_subset", header=True, inferSchema=True)

# Force Spark to combine everything into one partition
df_single = df.coalesce(1)

# Write as ONE CSV file
output_path = "/lakehouse/default/Files/granular_subset_single"

df_single.write.mode("overwrite").option("header", True).csv(output_path)
