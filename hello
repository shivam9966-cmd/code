# Read the granular subset (all part files)
df = (
    spark.read
    .option("header", True)
    .csv("/lakehouse/default/Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset/*")
)

# Combine into one partition
df_single = df.coalesce(1)

# Write as a single CSV file
output_path = "/lakehouse/default/Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single"

df_single.write.mode("overwrite").option("header", True).csv(output_path)
