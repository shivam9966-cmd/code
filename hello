
BASE_DIR = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
INPUT_PATH  = f"{BASE_DIR}/sample5000"                       
OUTPUT_PATH = f"{BASE_DIR}/anomaly_report_sample5000_delta"  

GROUP_COLS  = ['org','country','state','dc','product','sku']
DATE_COL    = 'date'
METRIC_COL  = 'amount'
METRIC_NAME = 'Revenue'
METHOD      = 'mad'     # or 'std'
HIST_WINDOW = 14
K           = 3.0
MIN_POINTS  = 7
LEVEL_TAG   = "org-**-sku"

ALIASES = {
    'date':   ['date','txn_date','posting_date','period_date'],
    'amount': ['amount','value','revenue','metric','amt']
}

from pyspark.sql import functions as F

def _first_present(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    return None

def _load_any(path):
    # Try Delta → Parquet → CSV
    try:
        return spark.read.format("delta").load(path)
    except Exception:
        pass
    try:
        return spark.read.parquet(path)
    except Exception:
        pass
    try:
        return (spark.read
                .option("header", True)
                .option("inferSchema", True)
                .csv(path))
    except Exception as e:
        raise RuntimeError(f"Could not load data from {path}. Tried delta/parquet/csv. Root error: {e}")

sample_df = _load_any(INPUT_PATH)

use_date   = DATE_COL  if DATE_COL  in sample_df.columns else _first_present(sample_df.columns, ALIASES['date'])
use_metric = METRIC_COL if METRIC_COL in sample_df.columns else _first_present(sample_df.columns, ALIASES['amount'])
if use_date is None or use_metric is None:
    raise ValueError(f"Could not resolve date/metric from aliases. Columns present: {sample_df.columns}")

missing_dims = [c for c in GROUP_COLS if c not in sample_df.columns]
if missing_dims:
    raise ValueError(f"Missing grouping columns: {missing_dims}")


anomaly_report_df = detect_anomalies_dynamic_threshold(
    df=sample_df,
    group_cols=GROUP_COLS,
    date_col=use_date,
    metric_col=use_metric,
    metric_name=METRIC_NAME,
    method=METHOD,
    hist_window=HIST_WINDOW,
    k=K,
    min_points=MIN_POINTS,
    level_tag=LEVEL_TAG
)


(anomaly_report_df
 .write
 .mode("overwrite")
 .format("delta")
 .save(OUTPUT_PATH))


try:
    display(anomaly_report_df.orderBy(F.col("predictive_date").desc()).limit(50))
except NameError:
    
    anomaly_report_df.orderBy(F.col("predictive_date").desc()).show(50, truncate=False)

print(f"Saved Delta anomaly report to: {OUTPUT_PATH}")
print(f"Anomaly rows: {anomaly_report_df.count()}")
