# =========================
# Imports
# =========================
import pyspark.sql.functions as F
from functools import reduce  # for combining filter conditions

# =========================
# Load facts & dimensions
# =========================
actual_df   = spark.table("FPNA_FISRPT_SILVER.dbo.ACTUALS")
plan_df     = spark.table("FPNA_FISRPT_SILVER.dbo.PLAN")
forecast_df = spark.table("FPNA_FISRPT_SILVER.dbo.FORECAST")

costcenter_df = spark.table("FPNA_FISRPT_SILVER.dbo.cost_center")
product_df    = spark.table("FPNA_FISRPT_SILVER.dbo.product")
account_df    = spark.table("FPNA_FISRPT_SILVER.dbo.account")

# Optional visibility:
# print("product_df");    product_df.printSchema()
# print("costcenter_df"); costcenter_df.printSchema()
# print("account_df");    account_df.printSchema()

# =========================
# Helpers
# =========================
def cols_except(df, exclude):
    ex = set(exclude)
    return [c for c in df.columns if c not in ex]

# ---------- A) SURFACE DUPLICATES (no mutation) ----------
def find_duplicates(df, fact_name):
    """
    Returns two DFs:
      exact_dups  : rows where *all* columns match and appear >1 time
      amount_only : groups where all non-Amount columns match but Amount differs
    """
    # 1) Exact duplicates (all columns)
    exact_dups = (
        df.groupBy(df.columns)
          .count()
          .filter(F.col("count") > 1)
          .withColumn("FACT_SOURCE", F.lit(fact_name))
    )

    # 2) "Only Amount differs" detection
    key_cols = cols_except(df, ["Amount"])
    amount_only = (
        df.groupBy(key_cols)
          .agg(
              F.count(F.lit(1)).alias("row_count"),
              F.countDistinct("Amount").alias("amount_variants"),
              F.sum("Amount").alias("summed_amount_for_key")
          )
          .filter(F.col("amount_variants") > 1)
          .withColumn("FACT_SOURCE", F.lit(fact_name))
    )
    return exact_dups, amount_only

# Run duplicate surfacing for visibility (inspect before proceeding)
actual_exact_dups,   actual_amount_only   = find_duplicates(actual_df,   "ACTUALS")
plan_exact_dups,     plan_amount_only     = find_duplicates(plan_df,     "PLAN")
forecast_exact_dups, forecast_amount_only = find_duplicates(forecast_df, "FORECAST")

# Quick temp views for inspection in Synapse
actual_exact_dups.createOrReplaceTempView("vw_exact_dups_actuals")
plan_exact_dups.createOrReplaceTempView("vw_exact_dups_plan")
forecast_exact_dups.createOrReplaceTempView("vw_exact_dups_forecast")

actual_amount_only.createOrReplaceTempView("vw_amount_only_dups_actuals")
plan_amount_only.createOrReplaceTempView("vw_amount_only_dups_plan")
forecast_amount_only.createOrReplaceTempView("vw_amount_only_dups_forecast")

# Example:
# spark.sql("SELECT * FROM vw_amount_only_dups_plan ORDER BY row_count DESC").show(50, False)

# ---------- B) CANONICAL DEDUPE (sum Amount when only Amount differs) ----------
def dedupe_sum_amount(df):
    key_cols = cols_except(df, ["Amount"])
    return df.groupBy(key_cols).agg(F.sum("Amount").alias("Amount"))

# ---------- C) ENRICH + KPI FILTER + FRONT-END FILTERS (schema-safe) ----------
def build_granular(fact_df, fact_name, scenarios=None, years=None, versions=None):
    """
    - Dedupes (sum Amount where only Amount differs)
    - Joins to dims (left)
    - Filters by SCENARIO/YEAR/VERSION (lists supported)
    - Keeps KPIs: Revenue, Operating Expenses, EBITDA (from hierarchy)
    - Adds FACT_SOURCE tag
    - Selects hierarchy columns *safely* (fills missing ones with NULL)
    """
    # 1) Dedup
    d = dedupe_sum_amount(fact_df).alias("d")

    # 2) Joins
    granular = (
        d.join(costcenter_df.alias("cc"), F.col("d.COSTCENTER")  == F.col("cc.CostCenter"),  "left")
         .join(product_df.alias("pd"),   F.col("d.PRODUCTLINE") == F.col("pd.ProductLine"), "left")
         .join(account_df.alias("ad"),   F.col("d.ACCOUNT")     == F.col("ad.Account"),     "left")
    )

    # 3) Filters (SCENARIO / YEAR / VERSION)
    conds = []
    if scenarios: conds.append(F.col("d.SCENARIO").isin([*scenarios]))
    if years:     conds.append(F.col("d.YEAR").isin([*years]))      # e.g., ["FY25"]
    if versions:  conds.append(F.col("d.VERSION").isin([*versions]))
    if conds:
        granular = granular.filter(reduce(lambda a, b: a & b, conds))

    # 4) KPI tagging from account hierarchy
    def contains_ebitda(colname):
        return F.upper(F.col(colname)).contains("EBITDA")

    kpi_expr = (
        F.when(F.col("ad.L5_Description") == "Revenue", "Revenue")
         .when(F.col("ad.L5_Description") == "Operating Expenses", "Operating Expenses")
         .when(
             (F.col("ad.L4") == "EBITDA") |
             contains_ebitda("ad.L5_Description") |
             contains_ebitda("ad.L6_Description") |
             contains_ebitda("ad.L7_Description"),
             "EBITDA"
         )
    )
    granular = granular.withColumn("KPI", kpi_expr).filter(F.col("KPI").isNotNull())

    # 5) Safe dynamic column selection
    existing_cols = set(granular.columns)

    def safe(colname, alias):
        return (F.col(colname).alias(alias)
                if colname in existing_cols
                else F.lit(None).alias(alias))

    granular = granular.select(
        # Product hierarchy (L1–L8; will be NULL if not present)
        safe("pd.L1_Description", "product_L1_Description"),
        safe("pd.L2_Description", "product_L2_Description"),
        safe("pd.L3_Description", "product_L3_Description"),
        safe("pd.L4_Description", "product_L4_Description"),
        safe("pd.L5_Description", "product_L5_Description"),
        safe("pd.L6_Description", "product_L6_Description"),
        safe("pd.L7_Description", "product_L7_Description"),
        safe("pd.L8_Description", "product_L8_Description"),

        # Account hierarchy (L1–L7; mix of codes/descriptions depending on your schema)
        safe("ad.L1", "account_L1"),
        safe("ad.L2", "account_L2"),
        safe("ad.L3", "account_L3"),
        safe("ad.L4", "account_L4"),
        safe("ad.L5_Description", "account_L5_Description"),
        safe("ad.L6_Description", "account_L6_Description"),
        safe("ad.L7_Description", "account_L7_Description"),

        # Cost center hierarchy (L1–L4)
        safe("cc.L1_Description", "cc_L1_Description"),
        safe("cc.L2_Description", "cc_L2_Description"),
        safe("cc.L3_Description", "cc_L3_Description"),
        safe("cc.L4_Description", "cc_L4_Description"),

        # Base keys / attributes
        F.col("d.ACCOUNT"), F.col("d.PRODUCTLINE"), F.col("d.COSTCENTER"),
        F.col("d.ENTITY"), F.col("d.CUSTOMER"), F.col("d.CURRENCY"),
        F.col("d.DATASOURCE"), F.col("d.DATE"),
        F.col("d.SCENARIO"), F.col("d.VERSION"), F.col("d.YEAR"),

        # Measure
        F.col("d.Amount"),

        # Tags
        F.col("KPI"),
        F.lit(fact_name).alias("FACT_SOURCE")
    )

    return granular

# =========================
# Build granular per fact (FY25)
# =========================
gran_actual   = build_granular(actual_df,   "ACTUALS",  years=["FY25"])
gran_plan     = build_granular(plan_df,     "PLAN",     years=["FY25"])
gran_forecast = build_granular(forecast_df, "FORECAST", years=["FY25"])

# =========================
# Union into one granular dataset
# =========================
granular_all = (
    gran_actual
      .unionByName(gran_plan, allowMissingColumns=True)
      .unionByName(gran_forecast, allowMissingColumns=True)
)

# =========================
# Persist to a distinct table
# =========================
spark.sql("CREATE DATABASE IF NOT EXISTS FPNA_FISRPT_GOLD")
target_table = "FPNA_FISRPT_GOLD.dbo.GRANULAR_KPI_FY25_ALL"  # distinct name as requested

(granular_all
 .write
 .mode("overwrite")
 .format("delta")   # swap to "parquet" if that's your standard
 .saveAsTable(target_table)
)

print(f"Written unified granular table to {target_table}")
