# Load the consolidated CSV file
consolidated_df = spark.read.csv("Files/SQL_DUMPS", header=True, inferSchema=True)

# Group by Account and count the number of rows per account
account_counts = consolidated_df.groupBy("ACCOUNT").count()

# Show the accounts with the least number of rows (ascending order)
account_counts.orderBy(F.col("count").asc()).show(20, truncate=False)

# Optional: total distinct accounts
distinct_accounts = account_counts.count()
print(f"Total distinct accounts: {distinct_accounts:,}")
