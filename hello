# Read all part files
df = (
    spark.read
    .option("header", True)
    .csv("Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset/*")
)

# Combine into single partition
df_single = df.coalesce(1)

# Delete output folder if it exists
dbutils.fs.rm("Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single", True)

# Write one CSV
df_single.write.mode("overwrite").option("header", True).csv(
    "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single"
)
