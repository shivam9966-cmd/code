

from pyspark.sql import functions as F, Window as W
from functools import reduce
from pyspark.sql import DataFrame


BASE_DIR    = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
HIST_WINDOW = 14
K_FACTOR    = 3.0
KPIS        = ["EBITDA", "OPEX", "Revenue"]

PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]
GROUP_COLS        = ACCOUNT_LEVELS + PRODUCT_LEVELS + COSTCENTER_LEVELS

raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

need_cols = PRODUCT_LEVELS + ACCOUNT_LEVELS + COSTCENTER_LEVELS + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION","Amount","YEAR","QUARTER"
]
df = raw_df.select([c for c in need_cols if c in raw_df.columns])


q_start_month = (F.when(F.col("QUARTER").cast("int")==1, F.lit(1))
                 .when(F.col("QUARTER").cast("int")==2, F.lit(4))
                 .when(F.col("QUARTER").cast("int")==3, F.lit(7))
                 .otherwise(F.lit(10)))
df = df.withColumn(
    "pred_date",
    F.to_date(F.concat_ws("-", F.col("YEAR").cast("int"), q_start_month, F.lit(1)), "yyyy-M-d")
)


def _value_block(cols):
    return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])

def _full_value_path():
    return F.concat_ws("-**-",
                       _value_block(ACCOUNT_LEVELS),
                       _value_block(PRODUCT_LEVELS),
                       _value_block(COSTCENTER_LEVELS))

def _aan_key(metric_name_lit):
    return F.concat_ws("::", metric_name_lit, _full_value_path())

# --- Recompute rolling stats for each KPI ---
def _recompute_for_kpi(df_base, kpi_label):
    l5 = F.lower(F.col("account_L5_Description"))

    if kpi_label == "OPEX":
        cond = l5 == F.lit("operating expense")
    else:
        cond = l5 == F.lit(kpi_label.lower())

    base = df_base.filter(cond).withColumn("aan_key", _aan_key(F.lit(kpi_label)))

    hist_w = (W.partitionBy(*GROUP_COLS)
              .orderBy("pred_date")
              .rowsBetween(-HIST_WINDOW, -1))

    base = (base
        .withColumn("rolling_mean", F.avg("Amount").over(hist_w))
        .withColumn("rolling_std",  F.stddev_samp("Amount").over(hist_w))
        .withColumn("lower_band",   F.col("rolling_mean") - F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("upper_band",   F.col("rolling_mean") + F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("recomp_delta",
            F.when((F.col("rolling_mean").isNotNull()) & (F.col("rolling_mean") != 0),
                   (F.col("Amount") - F.col("rolling_mean")) / F.abs(F.col("rolling_mean")) * 100.0))
        .select("aan_key","pred_date","Amount","rolling_mean","rolling_std",
                "lower_band","upper_band","recomp_delta")
        .withColumn("aan_name", F.lit(kpi_label))
    )
    return base

# --- Combine recomputed stats for all KPIs ---
recomputed_all = reduce(DataFrame.unionByName, [_recompute_for_kpi(df, k) for k in KPIS])

# --- Prepare anomaly data (combined_anoms must already exist) ---
anoms = (combined_anoms
         .withColumn("aan_delta_dbl", F.col("aan_delta").cast("double"))
         .select("aan","aan_name","predictive_date","aan_delta_dbl","aan_attr",
                 "aan_level","aan_value","inference","anomaly_id"))

# --- Join anomalies with recomputed stats ---
joined = (anoms.alias("a")
          .join(recomputed_all.alias("r"),
                (F.col("a.aan") == F.col("r.aan_key")) &
                (F.col("a.predictive_date") == F.col("r.pred_date")) &
                (F.col("a.aan_name") == F.col("r.aan_name")),
                "left"))

# --- Validation metrics ---
EPS_DELTA_PP = 0.05
EPS_MEAN     = 1e-6

validated = (joined
    .withColumn("delta_diff_pp", F.round(F.col("r.recomp_delta") - F.col("a.aan_delta_dbl"), 4))
    .withColumn("expected_diff", F.round(F.col("r.rolling_mean") - F.col("a.aan_attr"), 6))
    .withColumn("is_outside_band", (F.col("r.Amount") < F.col("r.lower_band")) | (F.col("r.Amount") > F.col("r.upper_band")))
    .withColumn("delta_ok", F.abs(F.col("delta_diff_pp")) <= F.lit(EPS_DELTA_PP))
    .withColumn("expected_ok", F.abs(F.col("expected_diff")) <= F.lit(EPS_MEAN))
    .withColumn("band_ok", F.col("is_outside_band"))
    .withColumn("overall_ok", F.col("delta_ok") & F.col("expected_ok") & F.col("band_ok"))
)

# --- Summary of validation ---
summary = (validated.groupBy("a.aan_name")
           .agg(F.count("*").alias("anomaly_rows"),
                F.sum(F.when(F.col("overall_ok"), 1).otherwise(0)).alias("verified_rows"),
                (F.sum(F.when(F.col("overall_ok"), 1).otherwise(0))/F.count("*")*100).alias("verified_pct"))
           .orderBy("a.aan_name"))
display(summary)

# --- Drilldown: mismatches ---
issues = (validated.filter(~F.col("overall_ok"))
          .select(
              F.col("a.aan_name").alias("kpi"),
              F.col("a.predictive_date").alias("date"),
              "a.aan_level","a.aan_value",
              F.col("r.Amount").alias("actual"),
              F.col("r.rolling_mean").alias("recomp_baseline"),
              F.col("r.lower_band").alias("recomp_lower"),
              F.col("r.upper_band").alias("recomp_upper"),
              F.col("a.aan_attr").alias("reported_baseline"),
              F.col("a.aan_delta_dbl").alias("reported_delta_pct"),
              F.col("r.recomp_delta").alias("recomp_delta_pct"),
              "delta_diff_pp","expected_diff","is_outside_band",
              "delta_ok","expected_ok","band_ok",
              "a.inference","a.anomaly_id"
          )
          .orderBy(F.col("date").desc(), F.abs(F.col("delta_diff_pp")).desc()))
display(issues.limit(200))
