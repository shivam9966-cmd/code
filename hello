# Read the file you just created
df_final = spark.read.parquet("Files/SQL_DUMPS/FACTS_CONCAT_DEDUP_JOINED_KPI")

# Show first 50 rows
df_final.show(50, truncate=False)






print("Row count:", df_final.count())
print("Columns:", df_final.columns)


df_final.groupBy("FACT_SOURCE").count().show()


df_final.select("KPI", "FACT_SOURCE", "Amount").distinct().show(20)


(
    df_final.groupBy([c for c in df_final.columns if c != "Amount"])
    .count()
    .filter("count > 1")
    .show(20, truncate=False)
)
