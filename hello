from pyspark.sql import functions as F

df_filtered = (
    granular
    .withColumn("YEAR", F.year("DATE"))
    .filter(
        (F.col("YEAR").isin(2024, 2025)) |
        ((F.col("YEAR") == 2026) & (F.col("scenario") == "Planned"))
    )
)




tmp_path = "/lakehouse/default/Tables/tmp_filtered_parquet"

df_filtered.write.mode("overwrite").parquet(tmp_path)








import glob

parquet_files = glob.glob(tmp_path + "/*.parquet")
output_path = "/lakehouse/default/Files/filtered_dump.sql"

with open(output_path, "w", encoding="utf-8") as out:

    for pf in parquet_files:
        df_part = spark.read.parquet(pf)

        cols = df_part.columns
        cols_sql = ",".join(cols)

        for row in df_part.collect():
            vals = []
            for v in row:
                v = "" if v is None else str(v).replace("'", "''")
                vals.append(f"'{v}'")

            out.write(
                f"INSERT INTO random_table ({cols_sql}) "
                f"VALUES ({','.join(vals)});\n"
            )

print("SQL dump created:", output_path)
