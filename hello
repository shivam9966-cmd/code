from pyspark.sql import functions as F

# Ensure DATE is converted to proper date
granular_df = granular_df.withColumn("DATE", F.to_date("DATE"))

# Filter June 2025
june_2025 = granular_df.filter(
    (F.year(F.col("DATE")) == 2025) &
    (F.month(F.col("DATE")) == 6)
)

# -----------------------------
# Revenue (L5 = "REV")
# -----------------------------
rev_june = june_2025.filter(F.col("Account_L5") == "REV") \
                    .agg(F.sum("Amount").alias("Revenue_June_2025"))

# -----------------------------
#  Operating Expenses (L5 = "OPEX")
# -----------------------------
opex_june = june_2025.filter(F.col("Account_L5") == "OPEX") \
                     .agg(F.sum("Amount").alias("OPEX_June_2025"))

# -----------------------------
#  COMPUTE EBITDA = REV - OPEX
# -----------------------------
ebitda_june = rev_june.crossJoin(opex_june) \
                       .withColumn("EBITDA_June_2025",
                                   F.col("Revenue_June_2025") - F.col("OPEX_June_2025"))

ebitda_june.show(truncate=False)
