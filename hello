# ================================================================
#  MOVING AVERAGE ANOMALY DETECTION (EBITDA, OPEX, REVENUE)
#  - Dynamic STD threshold
#  - aan_level/aan_value hierarchical cleanup
#  - Natural-language inference
#  - NO SAVE â€” only display results
# ================================================================

from pyspark.sql import functions as F, Window as W
from functools import reduce
from pyspark.sql import DataFrame

# ================================================================
# FUNCTION
# ================================================================
def detect_anomalies_mavg_std_report(
    df,
    account_levels,
    product_levels,
    costcenter_levels,
    date_col,
    metric_col,
    metric_name,
    hist_window=14,
    k=3.0,
    min_points=7,
):
    # ----- Helper: join non-null descriptions -----
    def join_non_null(cols):
        exprs = [F.col(c).cast("string") for c in cols]
        return F.concat_ws("-*-", *exprs)

    def join_level_names(cols):
        names = [c.replace("_Description", "") for c in cols]
        return F.concat_ws("-*-", *[F.lit(n) for n in names])

    # Build value and name paths (skip None gracefully)
    acct_values  = join_non_null(account_levels)
    prod_values  = join_non_null(product_levels)
    cc_values    = join_non_null(costcenter_levels)

    acct_levels  = join_level_names(account_levels)
    prod_levels  = join_level_names(product_levels)
    cc_levels    = join_level_names(costcenter_levels)

    # Prefix hierarchy blocks and join with "-**-"
    aan_level_expr = F.concat_ws(
        "-**-",
        F.concat_ws("-*-", F.lit("Account"), acct_levels),
        F.concat_ws("-*-", F.lit("ProductLine"), prod_levels),
        F.concat_ws("-*-", F.lit("CostCenter"), cc_levels)
    )

    aan_value_expr = F.concat_ws(
        "-**-",
        acct_values,
        prod_values,
        cc_values
    )

    # Trim trailing separators if any
    aan_level_expr = F.regexp_replace(aan_level_expr, r"(-\*\*-$)|(-\*-$)", "")
    aan_value_expr = F.regexp_replace(aan_value_expr, r"(-\*\*-$)|(-\*-$)", "")

    # Partition by full depth
    group_cols = account_levels + product_levels + costcenter_levels
    hist_w = (
        W.partitionBy(*group_cols)
         .orderBy(F.col(date_col))
         .rowsBetween(-hist_window, -1)
    )

    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    floor = F.lit(1e-9)
    band_half = F.greatest(F.coalesce(std_hist, F.lit(0.0)), floor) * F.lit(k)
    lower = mean_hist - band_half
    upper = mean_hist + band_half
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

    delta_num = F.when(
        mean_hist.isNotNull() & (mean_hist != 0),
        (F.col(metric_col) - mean_hist) / F.abs(mean_hist) * F.lit(100.0)
    ).otherwise(F.lit(None).cast("double"))

    aan_expr = F.concat_ws("::", F.lit(metric_name), aan_value_expr)

    out = (
        df
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("_delta_num", F.round(delta_num, 3))
        .withColumn("aan_level", aan_level_expr)
        .withColumn("aan_value", aan_value_expr)
        .withColumn("aan_name", F.lit(metric_name))
        .withColumn("aan", aan_expr)
        .withColumn(
            "is_anomaly",
            ((F.col(metric_col) < lower) | (F.col(metric_col) > upper)) &
            (cnt_hist >= F.lit(min_points))
        )
    )

    # Ranking and inference
    rank_w = W.partitionBy("predictive_date", "aan_name").orderBy(F.desc(F.abs(F.col("_delta_num"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    # Clean, human-readable inference
    inference_expr = F.concat_ws(
        " ",
        F.lit(metric_name), F.lit("anomaly: deviation"),
        F.format_string("%.1f%%", F.col("_delta_num")),
        F.lit("at Account="), F.col(account_levels[-1]),
        F.lit(", ProductLine="), F.col(product_levels[-1]),
        F.lit(", CostCenter="), F.col(costcenter_levels[-1]),
        F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
        F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
        F.lit("(band:"), F.format_string("%.2f", F.col("_lower")),
        F.lit("to"), F.format_string("%.2f", F.col("_upper")), F.lit(")")
    )

    result = (
        out.filter(F.col("is_anomaly"))
           .select(
               "aan",
               "aan_date",
               "predictive_date",
               "aan_level",
               F.format_string("%.3f", F.col("_delta_num")).alias("aan_delta"),
               F.col("_expected").alias("aan_attr"),
               F.col("_rank").cast("string").alias("aan_ranking"),
               inference_expr.alias("inference"),
               "aan_name",
               "aan_value",
               F.concat_ws("|",
                   F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                   F.col("aan_name"),
                   F.col("aan")
               ).alias("anomaly_id"),
               F.lit("Point Anomaly").alias("anomaly_type"),
               F.lit(False).alias("rca_present"),
               F.lit(False).alias("impact_present")
           )
    )
    return result


# ================================================================
# CALLER (EBITDA + OPEX + REVENUE, NO SAVE)
# ================================================================

BASE_DIR     = "Files/SQL_DUMPS_SMALL_ACCOUNTS"

PRODUCT_LEVELS   = ["product_L3_Description","product_L4_Description","product_L5_Description",
                    "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS   = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS= ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

REQUIRED_COLS = PRODUCT_LEVELS + ACCOUNT_LEVELS + COSTCENTER_LEVELS + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION","Amount","YEAR","QUARTER"
]

# ---- Load CSV folder ----
raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

df = raw_df.select([c for c in REQUIRED_COLS if c in raw_df.columns])

# ---- Synthetic date (YEAR + QUARTER) ----
q_start_month = F.when(F.col("QUARTER").cast("int")==1, F.lit(1)) \
                 .when(F.col("QUARTER").cast("int")==2, F.lit(4)) \
                 .when(F.col("QUARTER").cast("int")==3, F.lit(7)) \
                 .otherwise(F.lit(10))
df = df.withColumn(
    "pred_date",
    F.to_date(F.concat_ws("-", F.col("YEAR").cast("int"), q_start_month, F.lit(1)), "yyyy-M-d")
)

# ---- Run for each KPI ----
def run_kpi(label):
    kpi_df = df.filter(F.col("account_L5_Description") == F.lit(label))
    return detect_anomalies_mavg_std_report(
        df=kpi_df,
        account_levels=ACCOUNT_LEVELS,
        product_levels=PRODUCT_LEVELS,
        costcenter_levels=COSTCENTER_LEVELS,
        date_col="pred_date",
        metric_col="Amount",
        metric_name=label,
        hist_window=14,
        k=3.0,
        min_points=7,
    )

ebitda_anoms = run_kpi("EBITDA")
opex_anoms   = run_kpi("OPEX")
rev_anoms    = run_kpi("Revenue")

combined_anoms = reduce(DataFrame.unionByName, [ebitda_anoms, opex_anoms, rev_anoms])

display(combined_anoms.orderBy(F.col("predictive_date").desc()).limit(100))
print(f"Total anomalies across EBITDA, OPEX, and Revenue: {combined_anoms.count()}")
