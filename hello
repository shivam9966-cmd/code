# ===== CONFIG =====
BASE_DIR     = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
OUTPUT_DELTA = f"{BASE_DIR}/anomaly_report_sample5000_delta"

# Hierarchies aligned to your screenshot
PRODUCT_HIER = [
    "product_L3_Description",
    "product_L4_Description",
    "product_L5_Description",
    "product_L6_Description",
    "product_L7_Description",
    "product_L8_Description"
]

ACCOUNT_HIER = [
    "account_L5_Description",
    "account_L6_Description",
    "account_L7_Description"
]

COSTCNTR_HIER = [
    "CC_L1_Description",
    "CC_L2_Description",
    "CC_L3_Description",
    "CC_L4_Description"
]

HIERARCHIES  = [ACCOUNT_HIER, PRODUCT_HIER, COSTCNTR_HIER]

DATE_COL     = "YEAR"       # proxy for time since file uses YEAR/QUARTER
METRIC_COL   = "Amount"
METRIC_NAME  = "Revenue"
HIST_WINDOW  = 4            # since YEAR is coarse, you can also group by QUARTER if needed
K_FACTOR     = 3.0
MIN_POINTS   = 3
LEVEL_TAG    = "acct#**#prod#**#cc"

from pyspark.sql import functions as F

# ===== LOAD CSV-in-FOLDER =====
sample_df = (
    spark.read
        .format("csv")
        .option("header", True)
        .option("inferSchema", True)
        .option("recursiveFileLookup", "true")
        .load(BASE_DIR)
)

# ===== FILTER TO REQUIRED COLUMNS =====
required_cols = PRODUCT_HIER + ACCOUNT_HIER + COSTCNTR_HIER + [
    "ACCOUNT", "COSTCENTER", "PRODUCTLINE", "SCENARIO", "VERSION",
    "Amount", "YEAR", "QUARTER"
]
sample_df = sample_df.select([c for c in required_cols if c in sample_df.columns])

# ===== CALL FUNCTION =====
anomaly_report_df = detect_anomalies_mavg_std(
    df=sample_df,
    hierarchies=HIERARCHIES,
    date_col=DATE_COL,
    metric_col=METRIC_COL,
    metric_name=METRIC_NAME,
    hist_window=HIST_WINDOW,
    k=K_FACTOR,
    min_points=MIN_POINTS,
    level_tag=LEVEL_TAG
)

# ===== SAVE OUTPUT =====
(anomaly_report_df
 .write
 .mode("overwrite")
 .format("delta")
 .save(OUTPUT_DELTA))

display(anomaly_report_df.limit(50))
print(f"Saved Delta anomaly report to: {OUTPUT_DELTA}")
print(f"Rows flagged as anomalies: {anomaly_report_df.count()}")
