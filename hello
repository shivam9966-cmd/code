# === CONFIG ===
BASE_DIR   = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
INPUT_PATH = f"{BASE_DIR}/sample5000"                    # adjust if the actual subpath/file differs
OUTPUT_DELTA = f"{BASE_DIR}/anomaly_report_sample5000_delta"

# Map your hierarchies explicitly (edit names to match the consolidated file)
ACCOUNT_HIER = ['acct_L1','acct_L2','acct_L3','acct_L4','acct_L5','acct_L6','acct_L7']
PRODUCT_HIER = ['prod_L1','prod_L2','prod_L3','prod_L4']            # e.g., L1..L4
COSTCNTR_HIER= ['cc_L1','cc_L2','cc_L3','cc_L4']                    # e.g., L1..L4

# If your sample also has org/country/state/dc as “contextual” dims,
# add them into one of the hierarchies (or create a separate one).
# Example: prepend org/country/state/dc to ACCOUNT_HIER if they conceptually belong there.

HIERARCHIES = [ACCOUNT_HIER, PRODUCT_HIER, COSTCNTR_HIER]

DATE_COL    = 'date'         # change if your column is 'txn_date' etc.
METRIC_COL  = 'amount'       # change if it's 'value'/'revenue'
METRIC_NAME = 'Revenue'
HIST_WINDOW = 14
K_FACTOR    = 3.0
MIN_POINTS  = 7
LEVEL_TAG   = "acct#**#prod#**#cc"   # optional label visible in `aan_level`

# === LOADER (tries Delta -> Parquet -> CSV) ===
from pyspark.sql import functions as F

def _load_any(path):
    try:
        return spark.read.format("delta").load(path)
    except Exception:
        pass
    try:
        return spark.read.parquet(path)
    except Exception:
        pass
    try:
        return (spark.read
                .option("header", True)
                .option("inferSchema", True)
                .csv(path))
    except Exception as e:
        raise RuntimeError(f"Could not load data from {path}. Tried delta/parquet/csv. Root error: {e}")

# Quick directory check (uncomment to list)
# display(dbutils.fs.ls(BASE_DIR))

sample_df = _load_any(INPUT_PATH)

# Optional sanity: ensure required columns exist (will throw if missing)
missing = []
for grp in HIERARCHIES:
    for c in grp:
        if c not in sample_df.columns:
            missing.append(c)
for need in [DATE_COL, METRIC_COL]:
    if need not in sample_df.columns:
        missing.append(need)
if missing:
    raise ValueError(f"Missing columns in consolidated sample: {missing}")

# === CALL FUNCTION ===
anomaly_report_df = detect_anomalies_mavg_std(
    df=sample_df,
    hierarchies=HIERARCHIES,
    date_col=DATE_COL,
    metric_col=METRIC_COL,
    metric_name=METRIC_NAME,
    hist_window=HIST_WINDOW,
    k=K_FACTOR,
    min_points=MIN_POINTS,
    level_tag=LEVEL_TAG
)

# === SAVE (Delta) ===
(anomaly_report_df
 .write
 .mode("overwrite")
 .format("delta")
 .save(OUTPUT_DELTA))

# Optional: also register a table
# spark.sql("DROP TABLE IF EXISTS analytics.anomaly_report_sample5000")
# spark.sql(f"CREATE TABLE analytics.anomaly_report_sample5000 USING DELTA LOCATION '{OUTPUT_DELTA}'")

# Quick peek
try:
    display(anomaly_report_df.orderBy(F.col("predictive_date").desc()).limit(50))
except NameError:
    anomaly_report_df.orderBy(F.col("predictive_date").desc()).show(50, truncate=False)

print(f"Saved Delta anomaly report to: {OUTPUT_DELTA}")
print(f"Anomaly rows: {anomaly_report_df.count()}")
