# Read all part files
df = (
    spark.read
    .option("header", True)
    .csv("Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset/*")
)

# Combine into single partition
df_single = df.coalesce(1)

# Delete output folder if it exists (Fabric version)
mssparkutils.fs.rm(
    "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single",
    recurse=True
)

# Write as a single CSV
df_single.write.mode("overwrite").option("header", True).csv(
    "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_09-11-55/granular_subset_single"
)
