from pyspark.sql import functions as F

df_filtered = (
    granular
    .withColumn("YEAR", F.year("DATE"))
    .filter(
        (F.col("YEAR").isin(2024, 2025)) |
        ((F.col("YEAR") == 2026) & (F.col("scenario") == "Planned"))
    )
)




tmp_path = "Files/tmp_filtered_csv"

df_filtered.write.mode("overwrite").option("header", True).csv(tmp_path)





import glob

csv_files = glob.glob("/lakehouse/default/Files/tmp_filtered_csv/*.csv")
output_path = "/lakehouse/default/Files/filtered_dump.sql"

with open(output_path, "w", encoding="utf-8") as out:
    for file in csv_files:
        with open(file, "r", encoding="utf-8") as f:
            header = f.readline().strip().split(",")
            cols = ",".join(header)

            for line in f:
                values = line.strip().split(",")
                escaped = ["'" + v.replace("'", "''") + "'" for v in values]
                
                out.write(
                    f"INSERT INTO random_table ({cols}) "
                    f"VALUES ({','.join(escaped)});\n"
                )

print("SQL dump saved to:", output_path)
