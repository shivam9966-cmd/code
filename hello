from pyspark.sql import functions as F

# === 1ï¸âƒ£ Load the subset dataset ===
run_folder = "Files/SQL_DUMPS/FINAL_EXPORTS/Run_2025-11-10_21-35-42"  # ğŸ‘ˆ update timestamp if needed
subset_path = f"{run_folder}/granular_subset"

print(f"ğŸ“‚ Loading subset data from: {subset_path}")
subset = spark.read.option("header", True).csv(subset_path)
print(f"âœ… Subset loaded successfully with {subset.count()} rows")

# === 2ï¸âƒ£ Filter for this hierarchy ===
target_date = "2022-03-01"

df_check = (
    subset
    .filter((F.col("account_L5_Description") == "Operating Expenses") &
            (F.col("account_L6_Description") == "Variable Expense") &
            (F.col("account_L7_Description") == "Cost of Goods Sold") &
            (F.col("product_L3_Description") == "Banking Solutions") &
            (F.col("CC_L1_Description") == "All Cost Centers"))
    .withColumn("date", F.to_date("date"))
    .orderBy("date")
)

# === 3ï¸âƒ£ Show anomaly month + 3 months before ===
df_filtered = (
    df_check
    .filter(F.col("date") <= F.lit(target_date))
    .orderBy(F.col("date").desc())
    .limit(4)
)

print("âœ… Filtered data for the hierarchy:")
display(df_filtered.select("date", "Amount", "account_L5_Description",
                            "account_L6_Description", "account_L7_Description",
                            "product_L3_Description", "CC_L1_Description"))
