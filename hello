from pyspark.sql import functions as F

# --------------------------------------------------------
# 1. RENAME FACT COLUMNS TO LOWERCASE
# --------------------------------------------------------

rename_map = {
    "ACCOUNT": "account",
    "COSTCENTER": "costcenter",
    "PRODUCTLINE": "productline",
    "SCENARIO": "scenario",
    "VERSION": "version",
    "AMOUNT": "amount",
    "YEAR": "year",
    "QUARTER": "quarter",
    "DATE": "date"
}

for old, new in rename_map.items():
    if old in df_filtered.columns:
        df_filtered = df_filtered.withColumnRenamed(old, new)

# --------------------------------------------------------
# 2. CREATE actual_months & forecasted_months FROM SCENARIO
# --------------------------------------------------------

df_filtered = (
    df_filtered
    .withColumn(
        "actual_months",
        F.regexp_extract("scenario", r"F(\d+)_", 1).cast("int")
    )
    .withColumn(
        "forecasted_months",
        F.regexp_extract("scenario", r"_(\d+)$", 1).cast("int")
    )
)

# Fill NULLs with zero
df_filtered = df_filtered.fillna({
    "actual_months": 0,
    "forecasted_months": 0
})

# --------------------------------------------------------
# 3. KEEP ONLY THE FINAL 23 COLUMNS
# --------------------------------------------------------

final_cols = [
    "product_L3_Description",
    "product_L4_Description",
    "product_L5_Description",
    "product_L6_Description",
    "product_L7_Description",
    "product_L8_Description",

    "account_L5_Description",
    "account_L6_Description",
    "account_L7_Description",

    "CC_L1_Description",
    "CC_L2_Description",
    "CC_L3_Description",
    "CC_L4_Description",

    "account",
    "costcenter",
    "productline",
    "scenario",
    "actual_months",
    "forecasted_months",
    "version",
    "amount",
    "year",
    "quarter",
    "date"
]

df_final = df_filtered.select(*final_cols)

# Show result
df_final.show(5)
print("Columns:", df_final.columns)
print("Row count:", df_final.count())
