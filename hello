from pyspark.sql import functions as F

# === Your anomaly inputs ===
aan = "OPEX::Account-**-ProductLine-**-CostCenter-*-CC_L1-*-CC_L2-*-CC_L3-*-CC_L4-*"
predictive_date = "2022-03-01"
K_FACTOR = 2.5

# === Extract hierarchy info from AAN ===
metric = aan.split("::")[0]
acct = "Operating Expenses"  # use your account_L5_Description for OPEX
# Adjust if AAN includes more granular filters (like ProductLine, CostCenter, etc.)

# === Filter dataset to match that hierarchy ===
df_filt = subset.filter(F.col("account_L5_Description") == acct)

# === Get baseline (previous 3 months) ===
df_hist = df_filt.filter(F.col("date") < F.lit(predictive_date))
baseline = df_hist.agg(
    F.avg("Amount").alias("mean"),
    F.stddev("Amount").alias("std")
).collect()[0]

mean = baseline["mean"] or 0
std  = baseline["std"] or 0

# === Get the anomaly month’s amount ===
curr_val = (
    df_filt.filter(F.col("date") == F.lit(predictive_date))
    .select("Amount")
    .collect()
)
curr_val = curr_val[0]["Amount"] if curr_val else None

# === Calculate dynamic band & check ===
lower = mean - K_FACTOR * std
upper = mean + K_FACTOR * std
is_anom = curr_val < lower or curr_val > upper if curr_val is not None else None
delta_pct = ((curr_val - mean) / abs(mean)) * 100 if mean else 0

print(f"""
Manual Anomaly Validation:
--------------------------
AAN: {aan}
Metric: {metric}
Predictive Date: {predictive_date}
Current Value: {curr_val}
Baseline Mean: {mean:.2f}
Std Dev: {std:.2f}
Band: [{lower:.2f}, {upper:.2f}]
Δ%: {delta_pct:.2f}%
Anomaly? {'✅ YES' if is_anom else '❌ NO'}
""")
