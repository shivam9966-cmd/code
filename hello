from pyspark.sql import Window, functions as F

ACCOUNT_COL = "ACCOUNT"  # change if your column is named differently
TARGET = 5000

acct_counts = (
    df.groupBy(ACCOUNT_COL).count()
      .orderBy(F.col("count").asc(), F.col(ACCOUNT_COL).asc())
)

w = Window.orderBy(F.col("count").asc(), F.col(ACCOUNT_COL).asc())
acct_counts = (acct_counts
    .withColumn("row_number", F.row_number().over(w))
    .withColumn("cumulative_sum", F.sum("count").over(w))
)

exceed = (acct_counts
    .filter(F.col("cumulative_sum") > TARGET)
    .orderBy("cumulative_sum")
    .select("row_number")
    .limit(1)
    .collect()
)

if exceed:
    rnum = exceed[0][0]
    to_keep = acct_counts.where((F.col("cumulative_sum") <= TARGET) | (F.col("row_number") == rnum)) \
                         .select(ACCOUNT_COL).distinct()
else:
    to_keep = acct_counts.select(ACCOUNT_COL).distinct()

subset = df.join(to_keep, on=ACCOUNT_COL, how="inner")
print("Rows included (incl. full last account):", f"{subset.count():,}")
