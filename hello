from pyspark.sql import functions as F

# mapping: source column in df_clean  â†’  final name you want
col_map = [
    ("Product_L3_Description",   "product_L3_Description"),
    ("Product_L4_Description",   "product_L4_Description"),
    ("Product_L5_Description",   "product_L5_Description"),
    ("Product_L6_Description",   "product_L6_Description"),
    ("Product_L7_Description",   "product_L7_Description"),
    ("Product_L8_Description",   "product_L8_Description"),

    ("account_L5_Description",   "account_L5_Description"),
    ("account_L6_Description",   "account_L6_Description"),
    ("account_L7_Description",   "account_L7_Description"),

    ("CC_L1_Description",        "CC_L1_Description"),
    ("CC_L2_Description",        "CC_L2_Description"),
    ("CC_L3_Description",        "CC_L3_Description"),
    ("CC_L4_Description",        "CC_L4_Description"),

    ("ACCOUNT",                  "account"),
    ("COSTCENTER",               "costcenter"),
    ("PRODUCTLINE",              "productline"),
    ("SCENARIO",                 "scenario"),

    ("actual_months",            "actual_months"),
    ("forecasted_months",        "forecasted_months"),

    ("VERSION",                  "version"),
    ("Amount",                   "amount"),
    ("YEAR",                     "year"),
    ("QUARTER",                  "quarter")
]

# build the select list in the exact Excel order,
# only for columns that actually exist in df_clean
select_exprs = []
for src, tgt in col_map:
    if src in df_clean.columns:
        if src == tgt:
            select_exprs.append(F.col(src))
        else:
            select_exprs.append(F.col(src).alias(tgt))

df_final = df_clean.select(select_exprs)

df_final.columns
