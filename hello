from pyspark.sql import functions as F, Window as W

def detect_anomalies_mavg_std_report(
    df,
    hierarchies,            # list[list[str]] e.g. [ACCOUNT_HIER, PRODUCT_HIER, COSTCNTR_HIER]
    date_col,               # 'date' or a synthetic date; if YEAR/QUARTER given, create one before calling
    metric_col,             # 'Amount'
    metric_name,            # 'Revenue' | 'OPEX' | 'EBITDA'
    hist_window=14,
    k=3.0,
    min_points=7,
):
    """
    Outputs ONLY these columns:
      - aan (text)
      - aan_date (date)
      - predictive_date (date)
      - aan_level (text)
      - aan_delta (numeric as string)
      - aan_attr (numeric)           <- baseline (expected)
      - aan_ranking (text)           <- rank as string (1 = highest |aan_delta|)
      - inference (text)
      - aan_name (text)              <- metric_name
      - aan_value (text)             <- deepest entity path(s) matching aan_level
      - anomaly_id (text)            <- predictive_date|metric_name|aan
      - anomaly_type (text)          <- 'Point Anomaly'
      - rca_present (boolean)        <- default False
      - impact_present (boolean)     <- default False
    """
    # 1) Build grouping columns (all dims)
    group_cols = [c for grp in hierarchies for c in grp]

    # 2) Window: strictly past hist_window rows
    hist_w = (
        W.partitionBy(*group_cols)
         .orderBy(F.col(date_col))
         .rowsBetween(-hist_window, -1)
    )

    # 3) Moving average + dynamic STD band
    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    floor = F.lit(1e-9)
    band_half = F.greatest(F.coalesce(std_hist, F.lit(0.0)), floor) * F.lit(k)
    lower = mean_hist - band_half
    upper = mean_hist + band_half
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

    # 4) % deviation vs expected
    aan_delta_num = F.when(
        mean_hist.isNotNull() & (mean_hist != 0),
        (F.col(metric_col) - mean_hist) / F.abs(mean_hist) * F.lit(100.0)
    ).otherwise(F.lit(None).cast("double"))

    # 5) Build hierarchy paths:
    #    - within a hierarchy: '>' between levels
    #    - between hierarchies: '#**#' if multiple; '#*#' if single
    hier_paths = []
    for grp in hierarchies:
        if grp:
            path = F.concat_ws(">", *[F.coalesce(F.col(c).cast("string"), F.lit("∅")) for c in grp])
            hier_paths.append(path)

    inter_sep = "#**#" if len(hier_paths) > 1 else "#*#"

    if not hier_paths:
        combined_path = F.lit("∅")
        deepest_path  = F.lit("∅")
        level_tag_txt = F.lit("∅")
    else:
        combined_path = hier_paths[0]
        for p in hier_paths[1:]:
            combined_path = F.concat_ws("", combined_path, F.lit(inter_sep), p)

        # deepest members from each hierarchy = last column of each grp
        deepest_parts = []
        level_parts   = []
        for grp in hierarchies:
            last_col = grp[-1]
            deepest_parts.append(F.coalesce(F.col(last_col).cast("string"), F.lit("∅")))
            # level tag uses the column names to signal level (e.g., account_L7_Description → account_L7)
            level_parts.append(F.lit(last_col.replace("_Description","")))
        # aan_value = deepest entities per hierarchy, cross-joined by '#**#'
        deepest_path = deepest_parts[0]
        for p in deepest_parts[1:]:
            deepest_path = F.concat_ws("", deepest_path, F.lit("#**#"), p)
        # aan_level = lowest level labels per hierarchy joined by '-**-'
        level_tag_txt = level_parts[0]
        for p in level_parts[1:]:
            level_tag_txt = F.concat_ws("", level_tag_txt, F.lit("-**-"), p)

    # 6) AAN & other fields
    aan = F.concat_ws("::", F.lit(metric_name), combined_path)
    predictive_date = F.col(date_col).cast("date")
    aan_date = F.current_date()

    # anomaly flag
    is_anom = (
        (F.col(metric_col) < lower) | (F.col(metric_col) > upper)
    ) & (cnt_hist >= F.lit(min_points))

    out = (
        df
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("predictive_date", predictive_date)
        .withColumn("aan_date", aan_date)
        .withColumn("aan_delta_num", F.round(aan_delta_num, 3))
        .withColumn("aan", aan)
        .withColumn("aan_level", level_tag_txt)   # e.g., account_L7-**-product_L8-**-CC_L4
        .withColumn("aan_value", deepest_path)    # e.g., deepest entity(ies) matching aan_level
        .withColumn("aan_name", F.lit(metric_name))
        .withColumn("anomaly_type", F.lit("Point Anomaly"))
        .withColumn("rca_present", F.lit(False))
        .withColumn("impact_present", F.lit(False))
        .withColumn("is_anomaly", is_anom.cast("int"))
    )

    # 7) Rank by |aan_delta| per predictive_date & metric
    rank_w = W.partitionBy("predictive_date", "aan_name").orderBy(F.desc(F.abs(F.col("aan_delta_num"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    # 8) Keep only anomalies; format final columns
    result = (
        out.filter(F.col("is_anomaly") == 1)
           .select(
               F.col("aan"),
               F.col("aan_date"),
               F.col("predictive_date"),
               F.col("aan_level"),
               # cast delta to string to match "character varying" requirement
               F.format_string("%.3f", F.col("aan_delta_num")).alias("aan_delta"),
               F.col("_expected").alias("aan_attr"),
               F.col("_rank").cast("string").alias("aan_ranking"),
               # human-readable inference
               F.concat_ws(
                   " ",
                   F.lit(metric_name), F.lit("anomaly:"),
                   F.lit("deviation"), F.format_string("%.1f%%", F.col("aan_delta_num")),
                   F.lit("at"), F.col("aan_value"),
                   F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                   F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
                   F.lit("(band:"), F.format_string("%.2f", F.col("_lower")), F.lit("to"),
                   F.format_string("%.2f", F.col("_upper")), F.lit(")")
               ).alias("inference"),
               F.col("aan_name"),
               F.col("aan_value"),
               F.concat_ws("|",
                   F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                   F.lit(metric_name),
                   F.col("aan")
               ).alias("anomaly_id"),
               F.col("anomaly_type"),
               F.col("rca_present"),
               F.col("impact_present")
           )
    )
    return result
