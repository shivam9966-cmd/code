tmp_path = "/lakehouse/default/Files/tmp_filtered_parquet"

df_filtered.write.mode("overwrite").parquet(tmp_path)





import glob

parquet_files = glob.glob(tmp_path + "/*.parquet")
output_path = "/lakehouse/default/Files/filtered_dump.sql"

with open(output_path, "w", encoding="utf-8") as out:

    for pf in parquet_files:
        df_part = spark.read.parquet(pf)

        cols = df_part.columns
        cols_sql = ",".join(cols)

        for row in df_part.collect():
            vals = []
            for v in row:
                v = "" if v is None else str(v).replace("'", "''")
                vals.append(f"'{v}'")

            out.write(
                f"INSERT INTO random_table ({cols_sql}) VALUES ({','.join(vals)});\n"
            )

print("SQL dump created:", output_path)
