df = spark.read.parquet("Files/SQL_DUMPS/FACTS_CONCAT_DEDUP_JOINED_KPI")

# Force full column display (Spark truncates by default)
pd_df = df.limit(20).toPandas()

# Show *all* columns, no horizontal cutoff
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

display(pd_df)








print("Row count:", df_final.count())
print("Columns:", df_final.columns)


df_final.groupBy("FACT_SOURCE").count().show()


df_final.select("KPI", "FACT_SOURCE", "Amount").distinct().show(20)


(
    df_final.groupBy([c for c in df_final.columns if c != "Amount"])
    .count()
    .filter("count > 1")
    .show(20, truncate=False)
)
