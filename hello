

from pyspark.sql import functions as F, Window as W
from functools import reduce
from pyspark.sql import DataFrame


BASE_DIR    = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
HIST_WINDOW = 14
K_FACTOR    = 3.0

PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

REQUIRED_COLS = PRODUCT_LEVELS + ACCOUNT_LEVELS + COSTCENTER_LEVELS + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION","Amount","YEAR","QUARTER"
]


raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

df = raw_df.select([c for c in REQUIRED_COLS if c in raw_df.columns])

q_start_month = (F.when(F.col("QUARTER").cast("int")==1, F.lit(1))
                 .when(F.col("QUARTER").cast("int")==2, F.lit(4))
                 .when(F.col("QUARTER").cast("int")==3, F.lit(7))
                 .otherwise(F.lit(10)))
df = df.withColumn(
    "pred_date",
    F.to_date(F.concat_ws("-", F.col("YEAR").cast("int"), q_start_month, F.lit(1)), "yyyy-M-d")
)


def detect_anomalies_mavg_std_report(
    df_in,
    account_levels,
    product_levels,
    costcenter_levels,
    date_col,            # "pred_date"
    metric_col,          # "Amount"
    metric_name,         # "EBITDA" | "OPEX" | "Revenue"
    hist_window=14,
    k=3.0,
    min_points=7,
):
    # ----- helpers -----
    def join_non_null(cols):
        # within a hierarchy: values joined by "-*-", skipping nulls
        return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])

    def join_level_names(cols):
        # within a hierarchy: level names (col names w/o _Description) joined by "-*-"
        return F.concat_ws("-*-", *[F.lit(c.replace("_Description","")) for c in cols])

    def deepest_present(cols):
        # deepest non-null description for inference
        expr = None
        for c in reversed(cols):
            expr = F.when(F.col(c).isNotNull(), F.col(c).cast("string")).otherwise(expr)
        return expr

    # blocks (values & names)
    acct_vals = join_non_null(account_levels)
    prod_vals = join_non_null(product_levels)
    cc_vals   = join_non_null(costcenter_levels)

    acct_names = join_level_names(account_levels)
    prod_names = join_level_names(product_levels)
    cc_names   = join_level_names(costcenter_levels)

    # aan_level: "Account-*-acc_L5-*-acc_L6-*-acc_L7 -**- ProductLine-*-... -**- CostCenter-*-..."
    aan_level_expr = F.concat_ws(
        "-**-",
        F.concat_ws("-*-", F.lit("Account"),    acct_names),
        F.concat_ws("-*-", F.lit("ProductLine"),prod_names),
        F.concat_ws("-*-", F.lit("CostCenter"), cc_names)
    )
    # aan_value: "<acct values> -**- <prod values> -**- <cc values>"
    aan_value_expr = F.concat_ws("-**-", acct_vals, prod_vals, cc_vals)

    # partition by full depth (deepest granularity)
    group_cols = account_levels + product_levels + costcenter_levels
    hist_w = (W.partitionBy(*group_cols)
                .orderBy(F.col(date_col))
                .rowsBetween(-hist_window, -1))

    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    floor = F.lit(1e-9)
    band_half = F.greatest(F.coalesce(std_hist, F.lit(0.0)), floor) * F.lit(k)
    lower = mean_hist - band_half
    upper = mean_hist + band_half
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

    delta_num = F.when(
        mean_hist.isNotNull() & (mean_hist != 0),
        (F.col(metric_col) - mean_hist) / F.abs(mean_hist) * F.lit(100.0)
    ).otherwise(F.lit(None).cast("double"))

    # AAN key uses the value path (human-searchable)
    aan_key = F.concat_ws("::", F.lit(metric_name), aan_value_expr)

    # deepest for inference (skip if null via concat_ws later)
    acct_deep = deepest_present(account_levels)
    prod_deep = deepest_present(product_levels)
    cc_deep   = deepest_present(costcenter_levels)

    # natural-language inference parts (include only when present)
    acct_part = F.when(acct_deep.isNotNull(), F.concat(F.lit("Account="), acct_deep))
    prod_part = F.when(prod_deep.isNotNull(), F.concat(F.lit("ProductLine="), prod_deep))
    cc_part   = F.when(cc_deep.isNotNull(),   F.concat(F.lit("CostCenter="), cc_deep))

    out = (df_in
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("_delta_num", F.round(delta_num, 3))
        .withColumn("aan_level", aan_level_expr)
        .withColumn("aan_value", aan_value_expr)
        .withColumn("aan_name",  F.lit(metric_name))
        .withColumn("aan",       aan_key)
        .withColumn(
            "is_anomaly",
            ((F.col(metric_col) < lower) | (F.col(metric_col) > upper)) & (cnt_hist >= F.lit(min_points))
        )
    )

    # rank by |delta| within (date, metric)
    rank_w = W.partitionBy("predictive_date", "aan_name").orderBy(F.desc(F.abs(F.col("_delta_num"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    # inference sentence
    inference_expr = F.concat_ws(
        " ",
        F.lit(metric_name), F.lit("anomaly: deviation"),
        F.format_string("%.1f%%", F.col("_delta_num")),
        F.lit("at"),
        F.concat_ws(", ", acct_part, prod_part, cc_part),
        F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
        F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
        F.lit("(band:"), F.format_string("%.2f", F.col("_lower")),
        F.lit("to"), F.format_string("%.2f", F.col("_upper")), F.lit(")")
    )

    # final projection (only requested columns + 'date' alias)
    result = (out.filter(F.col("is_anomaly"))
        .select(
            "aan",
            "aan_date",
            "predictive_date",
            F.col("predictive_date").alias("date"),  # convenience column you asked for
            "aan_level",
            F.format_string("%.3f", F.col("_delta_num")).alias("aan_delta"),
            F.col("_expected").alias("aan_attr"),
            F.col("_rank").cast("string").alias("aan_ranking"),
            inference_expr.alias("inference"),
            "aan_name",
            "aan_value",
            F.concat_ws("|",
                F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                F.col("aan_name"),
                F.col("aan")
            ).alias("anomaly_id"),
            F.lit("Point Anomaly").alias("anomaly_type"),
            F.lit(False).alias("rca_present"),
            F.lit(False).alias("impact_present")
        )
    )
    return result

# =========================
# 4) RUN FOR KPIs (EBITDA, OPEX, REVENUE)
# =========================
def run_kpi(label):
    l5 = F.lower(F.col("account_L5_Description"))
    if label == "OPEX":
        kpi_df = df.filter(l5 == F.lit("operating expense"))  # your exact dataset label
    else:
        kpi_df = df.filter(l5 == F.lit(label.lower()))

    return detect_anomalies_mavg_std_report(
        df_in=kpi_df,
        account_levels=ACCOUNT_LEVELS,
        product_levels=PRODUCT_LEVELS,
        costcenter_levels=COSTCENTER_LEVELS,
        date_col="pred_date",
        metric_col="Amount",
        metric_name=label,
        hist_window=HIST_WINDOW,
        k=K_FACTOR,
        min_points=7,
    )

ebitda_anoms = run_kpi("EBITDA")
opex_anoms   = run_kpi("OPEX")
revenue_anoms= run_kpi("Revenue")

anomalies_df = reduce(DataFrame.unionByName, [ebitda_anoms, opex_anoms, revenue_anoms])

# Peek (NO SAVE)
display(anomalies_df.orderBy(F.col("predictive_date").desc()).limit(100))
print(f"Counts → EBITDA: {ebitda_anoms.count()}, OPEX: {opex_anoms.count()}, Revenue: {revenue_anoms.count()}, All: {anomalies_df.count()}")

# =========================
# 5) VALIDATE ANOMALIES against raw data
# =========================
# Helpers to rebuild the SAME 'aan' key from raw df
def _value_block(cols):
    return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])

def _full_value_path():
    return F.concat_ws("-**-", _value_block(ACCOUNT_LEVELS), _value_block(PRODUCT_LEVELS), _value_block(COSTCENTER_LEVELS))

def _aan_key(metric_name_lit):
    return F.concat_ws("::", metric_name_lit, _full_value_path())

GROUP_COLS = ACCOUNT_LEVELS + PRODUCT_LEVELS + COSTCENTER_LEVELS
KPIS = ["EBITDA", "OPEX", "Revenue"]

def _recompute_for_kpi(df_base, kpi_label):
    l5 = F.lower(F.col("account_L5_Description"))
    if kpi_label == "OPEX":
        cond = l5 == F.lit("operating expense")
    else:
        cond = l5 == F.lit(kpi_label.lower())

    base = df_base.filter(cond).withColumn("aan_key", _aan_key(F.lit(kpi_label)))

    hist_w = (W.partitionBy(*GROUP_COLS).orderBy("pred_date").rowsBetween(-HIST_WINDOW, -1))
    base = (base
        .withColumn("rolling_mean", F.avg("Amount").over(hist_w))
        .withColumn("rolling_std",  F.stddev_samp("Amount").over(hist_w))
        .withColumn("lower_band",   F.col("rolling_mean") - F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("upper_band",   F.col("rolling_mean") + F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("recomp_delta",
            F.when((F.col("rolling_mean").isNotNull()) & (F.col("rolling_mean") != 0),
                   (F.col("Amount")-F.col("rolling_mean"))/F.abs(F.col("rolling_mean"))*100.0))
        .select("aan_key","pred_date","Amount","rolling_mean","rolling_std","lower_band","upper_band","recomp_delta")
        .withColumn("aan_name", F.lit(kpi_label))
    )
    return base

recomputed_all = reduce(DataFrame.unionByName, [_recompute_for_kpi(df, k) for k in KPIS])

# align anomaly dtypes
anoms = (anomalies_df
         .withColumn("aan_delta_dbl", F.col("aan_delta").cast("double"))
         .select("aan","aan_name","predictive_date","aan_delta_dbl","aan_attr",
                 "aan_level","aan_value","inference","anomaly_id"))

# join by (aan, date, kpi)
joined = (anoms.alias("a")
          .join(recomputed_all.alias("r"),
                (F.col("a.aan") == F.col("r.aan_key")) &
                (F.col("a.predictive_date") == F.col("r.pred_date")) &
                (F.col("a.aan_name") == F.col("r.aan_name")),
                "left"))

# compare with tolerances
EPS_DELTA_PP = 0.05
EPS_MEAN     = 1e-6

validated = (joined
    .withColumn("delta_diff_pp", F.round(F.col("r.recomp_delta") - F.col("a.aan_delta_dbl"), 4))
    .withColumn("expected_diff", F.round(F.col("r.rolling_mean") - F.col("a.aan_attr"), 6))
    .withColumn("is_outside_band", (F.col("r.Amount") < F.col("r.lower_band")) | (F.col("r.Amount") > F.col("r.upper_band")))
    .withColumn("delta_ok", F.abs(F.col("delta_diff_pp")) <= F.lit(EPS_DELTA_PP))
    .withColumn("expected_ok", F.abs(F.col("expected_diff")) <= F.lit(EPS_MEAN))
    .withColumn("band_ok", F.col("is_outside_band"))
    .withColumn("overall_ok", F.col("delta_ok") & F.col("expected_ok") & F.col("band_ok"))
)

# summary + drilldown
summary = (validated.groupBy("a.aan_name")
           .agg(F.count("*").alias("anomaly_rows"),
                F.sum(F.when(F.col("overall_ok"), 1).otherwise(0)).alias("verified_rows"),
                (F.sum(F.when(F.col("overall_ok"), 1).otherwise(0))/F.count("*")*100).alias("verified_pct"))
           .orderBy("a.aan_name"))
display(summary)

issues = (validated.filter(~F.col("overall_ok"))
          .select(
              F.col("a.aan_name").alias("kpi"),
              F.col("a.predictive_date").alias("date"),
              "a.aan_level","a.aan_value",
              F.col("r.Amount").alias("actual"),
              F.col("r.rolling_mean").alias("recomp_baseline"),
              F.col("r.lower_band").alias("recomp_lower"),
              F.col("r.upper_band").alias("recomp_upper"),
              F.col("a.aan_attr").alias("reported_baseline"),
              F.col("a.aan_delta_dbl").alias("reported_delta_pct"),
              F.col("r.recomp_delta").alias("recomp_delta_pct"),
              "delta_diff_pp","expected_diff","is_outside_band",
              "delta_ok","expected_ok","band_ok",
              "a.inference","a.anomaly_id"
          )
          .orderBy(F.col("date").desc(), F.abs(F.col("delta_diff_pp")).desc()))
display(issues.limit(200))
# ================================================================
# END-TO-END: Generate anomalies (EBITDA, OPEX, Revenue) and validate them
# Uses saved dataset in Files/SQL_DUMPS_SMALL_ACCOUNTS
# - Dynamic threshold (rolling mean ± k * rolling std)
# - aan_level / aan_value in your exact format
# - Natural-language inference
# - NO SAVES (display only)
# - Uses name: anomalies_df  (replaces "combined_anoms")
# ================================================================

from pyspark.sql import functions as F, Window as W
from functools import reduce
from pyspark.sql import DataFrame

# =========================
# 1) CONFIG
# =========================
BASE_DIR    = "Files/SQL_DUMPS_SMALL_ACCOUNTS"
HIST_WINDOW = 14
K_FACTOR    = 3.0

PRODUCT_LEVELS    = ["product_L3_Description","product_L4_Description","product_L5_Description",
                     "product_L6_Description","product_L7_Description","product_L8_Description"]
ACCOUNT_LEVELS    = ["account_L5_Description","account_L6_Description","account_L7_Description"]
COSTCENTER_LEVELS = ["CC_L1_Description","CC_L2_Description","CC_L3_Description","CC_L4_Description"]

REQUIRED_COLS = PRODUCT_LEVELS + ACCOUNT_LEVELS + COSTCENTER_LEVELS + [
    "ACCOUNT","COSTCENTER","PRODUCTLINE","SCENARIO","VERSION","Amount","YEAR","QUARTER"
]

# =========================
# 2) LOAD DATASET & BUILD DATE
# =========================
raw_df = (spark.read
          .format("csv")
          .option("header", True)
          .option("inferSchema", True)
          .option("recursiveFileLookup", "true")
          .load(BASE_DIR))

df = raw_df.select([c for c in REQUIRED_COLS if c in raw_df.columns])

q_start_month = (F.when(F.col("QUARTER").cast("int")==1, F.lit(1))
                 .when(F.col("QUARTER").cast("int")==2, F.lit(4))
                 .when(F.col("QUARTER").cast("int")==3, F.lit(7))
                 .otherwise(F.lit(10)))
df = df.withColumn(
    "pred_date",
    F.to_date(F.concat_ws("-", F.col("YEAR").cast("int"), q_start_month, F.lit(1)), "yyyy-M-d")
)

# =========================
# 3) DETECTOR FUNCTION
# =========================
def detect_anomalies_mavg_std_report(
    df_in,
    account_levels,
    product_levels,
    costcenter_levels,
    date_col,            # "pred_date"
    metric_col,          # "Amount"
    metric_name,         # "EBITDA" | "OPEX" | "Revenue"
    hist_window=14,
    k=3.0,
    min_points=7,
):
    # ----- helpers -----
    def join_non_null(cols):
        # within a hierarchy: values joined by "-*-", skipping nulls
        return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])

    def join_level_names(cols):
        # within a hierarchy: level names (col names w/o _Description) joined by "-*-"
        return F.concat_ws("-*-", *[F.lit(c.replace("_Description","")) for c in cols])

    def deepest_present(cols):
        # deepest non-null description for inference
        expr = None
        for c in reversed(cols):
            expr = F.when(F.col(c).isNotNull(), F.col(c).cast("string")).otherwise(expr)
        return expr

    # blocks (values & names)
    acct_vals = join_non_null(account_levels)
    prod_vals = join_non_null(product_levels)
    cc_vals   = join_non_null(costcenter_levels)

    acct_names = join_level_names(account_levels)
    prod_names = join_level_names(product_levels)
    cc_names   = join_level_names(costcenter_levels)

    # aan_level: "Account-*-acc_L5-*-acc_L6-*-acc_L7 -**- ProductLine-*-... -**- CostCenter-*-..."
    aan_level_expr = F.concat_ws(
        "-**-",
        F.concat_ws("-*-", F.lit("Account"),    acct_names),
        F.concat_ws("-*-", F.lit("ProductLine"),prod_names),
        F.concat_ws("-*-", F.lit("CostCenter"), cc_names)
    )
    # aan_value: "<acct values> -**- <prod values> -**- <cc values>"
    aan_value_expr = F.concat_ws("-**-", acct_vals, prod_vals, cc_vals)

    # partition by full depth (deepest granularity)
    group_cols = account_levels + product_levels + costcenter_levels
    hist_w = (W.partitionBy(*group_cols)
                .orderBy(F.col(date_col))
                .rowsBetween(-hist_window, -1))

    mean_hist = F.avg(F.col(metric_col)).over(hist_w)
    std_hist  = F.stddev_samp(F.col(metric_col)).over(hist_w)
    floor = F.lit(1e-9)
    band_half = F.greatest(F.coalesce(std_hist, F.lit(0.0)), floor) * F.lit(k)
    lower = mean_hist - band_half
    upper = mean_hist + band_half
    cnt_hist = F.count(F.col(metric_col)).over(hist_w)

    delta_num = F.when(
        mean_hist.isNotNull() & (mean_hist != 0),
        (F.col(metric_col) - mean_hist) / F.abs(mean_hist) * F.lit(100.0)
    ).otherwise(F.lit(None).cast("double"))

    # AAN key uses the value path (human-searchable)
    aan_key = F.concat_ws("::", F.lit(metric_name), aan_value_expr)

    # deepest for inference (skip if null via concat_ws later)
    acct_deep = deepest_present(account_levels)
    prod_deep = deepest_present(product_levels)
    cc_deep   = deepest_present(costcenter_levels)

    # natural-language inference parts (include only when present)
    acct_part = F.when(acct_deep.isNotNull(), F.concat(F.lit("Account="), acct_deep))
    prod_part = F.when(prod_deep.isNotNull(), F.concat(F.lit("ProductLine="), prod_deep))
    cc_part   = F.when(cc_deep.isNotNull(),   F.concat(F.lit("CostCenter="), cc_deep))

    out = (df_in
        .withColumn("predictive_date", F.col(date_col).cast("date"))
        .withColumn("aan_date", F.current_date())
        .withColumn("_expected", mean_hist)
        .withColumn("_lower", lower)
        .withColumn("_upper", upper)
        .withColumn("_cnt_hist", cnt_hist)
        .withColumn("_delta_num", F.round(delta_num, 3))
        .withColumn("aan_level", aan_level_expr)
        .withColumn("aan_value", aan_value_expr)
        .withColumn("aan_name",  F.lit(metric_name))
        .withColumn("aan",       aan_key)
        .withColumn(
            "is_anomaly",
            ((F.col(metric_col) < lower) | (F.col(metric_col) > upper)) & (cnt_hist >= F.lit(min_points))
        )
    )

    # rank by |delta| within (date, metric)
    rank_w = W.partitionBy("predictive_date", "aan_name").orderBy(F.desc(F.abs(F.col("_delta_num"))))
    out = out.withColumn("_rank", F.dense_rank().over(rank_w))

    # inference sentence
    inference_expr = F.concat_ws(
        " ",
        F.lit(metric_name), F.lit("anomaly: deviation"),
        F.format_string("%.1f%%", F.col("_delta_num")),
        F.lit("at"),
        F.concat_ws(", ", acct_part, prod_part, cc_part),
        F.lit("on"), F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
        F.lit("vs baseline"), F.format_string("%.2f", F.col("_expected")),
        F.lit("(band:"), F.format_string("%.2f", F.col("_lower")),
        F.lit("to"), F.format_string("%.2f", F.col("_upper")), F.lit(")")
    )

    # final projection (only requested columns + 'date' alias)
    result = (out.filter(F.col("is_anomaly"))
        .select(
            "aan",
            "aan_date",
            "predictive_date",
            F.col("predictive_date").alias("date"),  # convenience column you asked for
            "aan_level",
            F.format_string("%.3f", F.col("_delta_num")).alias("aan_delta"),
            F.col("_expected").alias("aan_attr"),
            F.col("_rank").cast("string").alias("aan_ranking"),
            inference_expr.alias("inference"),
            "aan_name",
            "aan_value",
            F.concat_ws("|",
                F.date_format(F.col("predictive_date"), "yyyy-MM-dd"),
                F.col("aan_name"),
                F.col("aan")
            ).alias("anomaly_id"),
            F.lit("Point Anomaly").alias("anomaly_type"),
            F.lit(False).alias("rca_present"),
            F.lit(False).alias("impact_present")
        )
    )
    return result

# =========================
# 4) RUN FOR KPIs (EBITDA, OPEX, REVENUE)
# =========================
def run_kpi(label):
    l5 = F.lower(F.col("account_L5_Description"))
    if label == "OPEX":
        kpi_df = df.filter(l5 == F.lit("operating expense"))  # your exact dataset label
    else:
        kpi_df = df.filter(l5 == F.lit(label.lower()))

    return detect_anomalies_mavg_std_report(
        df_in=kpi_df,
        account_levels=ACCOUNT_LEVELS,
        product_levels=PRODUCT_LEVELS,
        costcenter_levels=COSTCENTER_LEVELS,
        date_col="pred_date",
        metric_col="Amount",
        metric_name=label,
        hist_window=HIST_WINDOW,
        k=K_FACTOR,
        min_points=7,
    )

ebitda_anoms = run_kpi("EBITDA")
opex_anoms   = run_kpi("OPEX")
revenue_anoms= run_kpi("Revenue")

anomalies_df = reduce(DataFrame.unionByName, [ebitda_anoms, opex_anoms, revenue_anoms])

# Peek (NO SAVE)
display(anomalies_df.orderBy(F.col("predictive_date").desc()).limit(100))
print(f"Counts → EBITDA: {ebitda_anoms.count()}, OPEX: {opex_anoms.count()}, Revenue: {revenue_anoms.count()}, All: {anomalies_df.count()}")

# =========================
# 5) VALIDATE ANOMALIES against raw data
# =========================
# Helpers to rebuild the SAME 'aan' key from raw df
def _value_block(cols):
    return F.concat_ws("-*-", *[F.col(c).cast("string") for c in cols])

def _full_value_path():
    return F.concat_ws("-**-", _value_block(ACCOUNT_LEVELS), _value_block(PRODUCT_LEVELS), _value_block(COSTCENTER_LEVELS))

def _aan_key(metric_name_lit):
    return F.concat_ws("::", metric_name_lit, _full_value_path())

GROUP_COLS = ACCOUNT_LEVELS + PRODUCT_LEVELS + COSTCENTER_LEVELS
KPIS = ["EBITDA", "OPEX", "Revenue"]

def _recompute_for_kpi(df_base, kpi_label):
    l5 = F.lower(F.col("account_L5_Description"))
    if kpi_label == "OPEX":
        cond = l5 == F.lit("operating expense")
    else:
        cond = l5 == F.lit(kpi_label.lower())

    base = df_base.filter(cond).withColumn("aan_key", _aan_key(F.lit(kpi_label)))

    hist_w = (W.partitionBy(*GROUP_COLS).orderBy("pred_date").rowsBetween(-HIST_WINDOW, -1))
    base = (base
        .withColumn("rolling_mean", F.avg("Amount").over(hist_w))
        .withColumn("rolling_std",  F.stddev_samp("Amount").over(hist_w))
        .withColumn("lower_band",   F.col("rolling_mean") - F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("upper_band",   F.col("rolling_mean") + F.lit(K_FACTOR)*F.col("rolling_std"))
        .withColumn("recomp_delta",
            F.when((F.col("rolling_mean").isNotNull()) & (F.col("rolling_mean") != 0),
                   (F.col("Amount")-F.col("rolling_mean"))/F.abs(F.col("rolling_mean"))*100.0))
        .select("aan_key","pred_date","Amount","rolling_mean","rolling_std","lower_band","upper_band","recomp_delta")
        .withColumn("aan_name", F.lit(kpi_label))
    )
    return base

recomputed_all = reduce(DataFrame.unionByName, [_recompute_for_kpi(df, k) for k in KPIS])

# align anomaly dtypes
anoms = (anomalies_df
         .withColumn("aan_delta_dbl", F.col("aan_delta").cast("double"))
         .select("aan","aan_name","predictive_date","aan_delta_dbl","aan_attr",
                 "aan_level","aan_value","inference","anomaly_id"))

# join by (aan, date, kpi)
joined = (anoms.alias("a")
          .join(recomputed_all.alias("r"),
                (F.col("a.aan") == F.col("r.aan_key")) &
                (F.col("a.predictive_date") == F.col("r.pred_date")) &
                (F.col("a.aan_name") == F.col("r.aan_name")),
                "left"))

# compare with tolerances
EPS_DELTA_PP = 0.05
EPS_MEAN     = 1e-6

validated = (joined
    .withColumn("delta_diff_pp", F.round(F.col("r.recomp_delta") - F.col("a.aan_delta_dbl"), 4))
    .withColumn("expected_diff", F.round(F.col("r.rolling_mean") - F.col("a.aan_attr"), 6))
    .withColumn("is_outside_band", (F.col("r.Amount") < F.col("r.lower_band")) | (F.col("r.Amount") > F.col("r.upper_band")))
    .withColumn("delta_ok", F.abs(F.col("delta_diff_pp")) <= F.lit(EPS_DELTA_PP))
    .withColumn("expected_ok", F.abs(F.col("expected_diff")) <= F.lit(EPS_MEAN))
    .withColumn("band_ok", F.col("is_outside_band"))
    .withColumn("overall_ok", F.col("delta_ok") & F.col("expected_ok") & F.col("band_ok"))
)

# summary + drilldown
summary = (validated.groupBy("a.aan_name")
           .agg(F.count("*").alias("anomaly_rows"),
                F.sum(F.when(F.col("overall_ok"), 1).otherwise(0)).alias("verified_rows"),
                (F.sum(F.when(F.col("overall_ok"), 1).otherwise(0))/F.count("*")*100).alias("verified_pct"))
           .orderBy("a.aan_name"))
display(summary)

issues = (validated.filter(~F.col("overall_ok"))
          .select(
              F.col("a.aan_name").alias("kpi"),
              F.col("a.predictive_date").alias("date"),
              "a.aan_level","a.aan_value",
              F.col("r.Amount").alias("actual"),
              F.col("r.rolling_mean").alias("recomp_baseline"),
              F.col("r.lower_band").alias("recomp_lower"),
              F.col("r.upper_band").alias("recomp_upper"),
              F.col("a.aan_attr").alias("reported_baseline"),
              F.col("a.aan_delta_dbl").alias("reported_delta_pct"),
              F.col("r.recomp_delta").alias("recomp_delta_pct"),
              "delta_diff_pp","expected_diff","is_outside_band",
              "delta_ok","expected_ok","band_ok",
              "a.inference","a.anomaly_id"
          )
          .orderBy(F.col("date").desc(), F.abs(F.col("delta_diff_pp")).desc()))
display(issues.limit(200))
