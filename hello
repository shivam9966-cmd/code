from pyspark.sql import functions as F

def filtered_data_analysis(dataset, amount_type, scenario, year, version):
    # Base join (same as your code)
    granular_df = (
        dataset.alias("d")
        .join(costcenter_df.alias("cc"), F.col("d.COSTCENTER") == F.col("cc.CostCenter"), "left")
        .join(product_df.alias("pd"), F.col("d.PRODUCTLINE") == F.col("pd.ProductLine"), "left")
        .join(account_df.alias("ad"), F.col("d.ACCOUNT") == F.col("ad.Account"), "left")
    )

    # ---------- ACCOUNT FILTER ----------
    # For EBITDA → use L4
    # For Revenue / OPEX → use L5_Description (as in your original)
    if amount_type == "EBITDA":
        account_condition = (F.col("ad.L4") == "EBITDA")
    else:
        account_condition = F.col("ad.L5_Description").isin(amount_type)

    # ---------- SCENARIO + YEAR + VERSION ----------
    scenario_condition = (
        F.col("d.SCENARIO").isin(scenario)
        & (F.col("d.YEAR") == year)
        & (F.col("d.VERSION") == version)
    )

    if scenario and year and version:
        granular_df = granular_df.filter(account_condition & scenario_condition)
    else:
        granular_df = granular_df.filter(account_condition)

    # ---------- SELECT (same fields you had) ----------
    granular_df = granular_df.select(
        F.col("pd.L3_Description").alias("product_L3_Description"),
        F.col("pd.L4_Description").alias("product_L4_Description"),
        F.col("pd.L5_Description").alias("product_L5_Description"),

        F.col("ad.L3_Description").alias("account_L3_Description"),
        F.col("ad.L4_Description").alias("account_L4_Description"),
        F.col("ad.L5_Description").alias("account_L5_Description"),
        F.col("ad.L6_Description").alias("account_L6_Description"),

        F.col("d.COSTCENTER"),
        F.col("d.PRODUCTLINE"),
        F.col("d.ACCOUNT"),
        F.col("d.SCENARIO"),
        F.col("d.VERSION"),
        F.col("d.Amount"),
        F.col("d.YEAR")
    )

    return granular_df
