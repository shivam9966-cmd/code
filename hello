# ===== CONFIG =====
BASE_DIR     = "Files/SQL_DUMPS_SMALL_ACCOUNTS"          # <- your folder from the screenshot
OUTPUT_DELTA = f"{BASE_DIR}/anomaly_report_sample5000_delta"

# Explicit hierarchies (rename columns to match your consolidated file)
ACCOUNT_HIER = ['acct_L1','acct_L2','acct_L3','acct_L4','acct_L5','acct_L6','acct_L7']
PRODUCT_HIER = ['prod_L1','prod_L2','prod_L3','prod_L4']
COSTCNTR_HIER= ['cc_L1','cc_L2','cc_L3','cc_L4']
HIERARCHIES  = [ACCOUNT_HIER, PRODUCT_HIER, COSTCNTR_HIER]

DATE_COL     = 'date'     # change if your file uses 'txn_date' etc.
METRIC_COL   = 'amount'   # change if it's 'value'/'revenue'
METRIC_NAME  = 'Revenue'
HIST_WINDOW  = 14
K_FACTOR     = 3.0
MIN_POINTS   = 7
LEVEL_TAG    = "acct#**#prod#**#cc"

from pyspark.sql import functions as F

# ===== LOAD (CSV-in-folder with part-00000) =====
# Your screenshot shows `_SUCCESS` + `part-00000-...` inside BASE_DIR.
# Use recursiveFileLookup so Spark picks the part file(s).
sample_df = (spark.read
             .format("csv")
             .option("header", True)
             .option("inferSchema", True)
             .option("recursiveFileLookup", "true")
             .load(BASE_DIR))

# (Optional) sanity check
# display(dbutils.fs.ls(BASE_DIR))
# sample_df.printSchema()
# sample_df.limit(5).show(truncate=False)

# Ensure required columns exist
missing = []
for grp in HIERARCHIES:
    for c in grp:
        if c not in sample_df.columns:
            missing.append(c)
for need in [DATE_COL, METRIC_COL]:
    if need not in sample_df.columns:
        missing.append(need)
if missing:
    raise ValueError(f"Missing columns in consolidated sample: {missing}")

# ===== RUN DETECTOR =====
anomaly_report_df = detect_anomalies_mavg_std(
    df=sample_df,
    hierarchies=HIERARCHIES,
    date_col=DATE_COL,
    metric_col=METRIC_COL,
    metric_name=METRIC_NAME,
    hist_window=HIST_WINDOW,
    k=K_FACTOR,
    min_points=MIN_POINTS,
    level_tag=LEVEL_TAG
)

# ===== SAVE (Delta) =====
(anomaly_report_df
 .write
 .mode("overwrite")
 .format("delta")
 .save(OUTPUT_DELTA))

# Optional: register a table
# spark.sql("DROP TABLE IF EXISTS analytics.anomaly_report_sample5000")
# spark.sql(f"CREATE TABLE analytics.anomaly_report_sample5000 USING DELTA LOCATION '{OUTPUT_DELTA}'")

# Quick peek
try:
    display(anomaly_report_df.orderBy(F.col("predictive_date").desc()).limit(50))
except NameError:
    anomaly_report_df.orderBy(F.col("predictive_date").desc()).show(50, truncate=False)

print(f"Saved Delta anomaly report to: {OUTPUT_DELTA}")
print(f"Anomaly rows: {anomaly_report_df.count()}")
